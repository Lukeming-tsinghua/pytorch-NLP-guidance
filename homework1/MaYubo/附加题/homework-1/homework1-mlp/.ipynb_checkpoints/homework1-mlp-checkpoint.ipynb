{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework-1: MLP for MNIST Classification\n",
    "\n",
    "### In this homework, you need to\n",
    "- #### implement SGD optimizer (`./optimizer.py`)\n",
    "- #### implement forward and backward for FCLayer (`layers/fc_layer.py`)\n",
    "- #### implement forward and backward for SigmoidLayer (`layers/sigmoid_layer.py`)\n",
    "- #### implement forward and backward for ReLULayer (`layers/relu_layer.py`)\n",
    "- #### implement EuclideanLossLayer (`criterion/euclidean_loss.py`)\n",
    "- #### implement SoftmaxCrossEntropyLossLayer (`criterion/softmax_cross_entropy.py`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "\n",
    "from network import Network\n",
    "from solver import train, test\n",
    "from plot import plot_loss_and_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load MNIST Dataset\n",
    "We use tensorflow tools to load dataset for convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_image(image):\n",
    "    # Normalize from [0, 255.] to [0., 1.0], and then subtract by the mean value\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image = tf.reshape(image, [784])\n",
    "    image = image / 255.0\n",
    "    image = image - tf.reduce_mean(image)\n",
    "    return image\n",
    "\n",
    "def decode_label(label):\n",
    "    # Encode label with one-hot encoding\n",
    "    return tf.one_hot(label, depth=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preprocessing\n",
    "x_train = tf.data.Dataset.from_tensor_slices(x_train).map(decode_image)\n",
    "y_train = tf.data.Dataset.from_tensor_slices(y_train).map(decode_label)\n",
    "data_train = tf.data.Dataset.zip((x_train, y_train))\n",
    "\n",
    "x_test = tf.data.Dataset.from_tensor_slices(x_test).map(decode_image)\n",
    "y_test = tf.data.Dataset.from_tensor_slices(y_test).map(decode_label)\n",
    "data_test = tf.data.Dataset.zip((x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Hyerparameters\n",
    "You can modify hyerparameters by yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "max_epoch = 20\n",
    "init_std = 0.01\n",
    "\n",
    "learning_rate_SGD = 0.001\n",
    "weight_decay = 0.1\n",
    "\n",
    "disp_freq = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. MLP with Euclidean Loss\n",
    "In part-1, you need to train a MLP with **Euclidean Loss**.  \n",
    "**Sigmoid Activation Function** and **ReLU Activation Function** will be used respectively.\n",
    "### TODO\n",
    "Before executing the following code, you should complete **./optimizer.py** and **criterion/euclidean_loss.py**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from criterion import EuclideanLossLayer\n",
    "from optimizer import SGD\n",
    "\n",
    "criterion = EuclideanLossLayer()\n",
    "\n",
    "sgd = SGD(learning_rate_SGD, weight_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 MLP with Euclidean Loss and Sigmoid Activation Function\n",
    "Build and train a MLP contraining one hidden layer with 128 units using Sigmoid activation function and Euclidean loss function.\n",
    "\n",
    "### TODO\n",
    "Before executing the following code, you should complete **layers/fc_layer.py** and **layers/sigmoid_layer.py**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from layers import FCLayer, SigmoidLayer\n",
    "\n",
    "sigmoidMLP = Network()\n",
    "# Build MLP with FCLayer and SigmoidLayer\n",
    "# 128 is the number of hidden units, you can change by your own\n",
    "sigmoidMLP.add(FCLayer(784, 128))\n",
    "sigmoidMLP.add(SigmoidLayer())\n",
    "sigmoidMLP.add(FCLayer(128, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Research\\Yu Lab\\NLP学习班\\作业\\第一次作业\\附加题\\homework-1\\homework1-mlp\\solver.py:14: DatasetV1.make_one_shot_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_one_shot_iterator(dataset)`.\n",
      "WARNING:tensorflow:From D:\\Research\\Yu Lab\\NLP学习班\\作业\\第一次作业\\附加题\\homework-1\\homework1-mlp\\solver.py:15: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\Research\\Yu Lab\\NLP学习班\\作业\\第一次作业\\附加题\\homework-1\\homework1-mlp\\solver.py:16: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "Epoch [0][20]\t Batch [0][550]\t Training Loss 3.8245\t Accuracy 0.0900\n",
      "Epoch [0][20]\t Batch [50][550]\t Training Loss 4.2662\t Accuracy 0.2208\n",
      "Epoch [0][20]\t Batch [100][550]\t Training Loss 2.3704\t Accuracy 0.3071\n",
      "Epoch [0][20]\t Batch [150][550]\t Training Loss 1.7098\t Accuracy 0.3816\n",
      "Epoch [0][20]\t Batch [200][550]\t Training Loss 1.3684\t Accuracy 0.4524\n",
      "Epoch [0][20]\t Batch [250][550]\t Training Loss 1.1585\t Accuracy 0.5034\n",
      "Epoch [0][20]\t Batch [300][550]\t Training Loss 1.0148\t Accuracy 0.5444\n",
      "Epoch [0][20]\t Batch [350][550]\t Training Loss 0.9104\t Accuracy 0.5757\n",
      "Epoch [0][20]\t Batch [400][550]\t Training Loss 0.8299\t Accuracy 0.6027\n",
      "Epoch [0][20]\t Batch [450][550]\t Training Loss 0.7659\t Accuracy 0.6247\n",
      "Epoch [0][20]\t Batch [500][550]\t Training Loss 0.7137\t Accuracy 0.6426\n",
      "\n",
      "Epoch [0]\t Average training loss 0.6706\t Average training accuracy 0.6585\n",
      "Epoch [0]\t Average validation loss 0.2110\t Average validation accuracy 0.8588\n",
      "\n",
      "Epoch [1][20]\t Batch [0][550]\t Training Loss 0.2122\t Accuracy 0.8500\n",
      "Epoch [1][20]\t Batch [50][550]\t Training Loss 0.2210\t Accuracy 0.8375\n",
      "Epoch [1][20]\t Batch [100][550]\t Training Loss 0.2185\t Accuracy 0.8387\n",
      "Epoch [1][20]\t Batch [150][550]\t Training Loss 0.2199\t Accuracy 0.8337\n",
      "Epoch [1][20]\t Batch [200][550]\t Training Loss 0.2174\t Accuracy 0.8354\n",
      "Epoch [1][20]\t Batch [250][550]\t Training Loss 0.2147\t Accuracy 0.8361\n",
      "Epoch [1][20]\t Batch [300][550]\t Training Loss 0.2125\t Accuracy 0.8386\n",
      "Epoch [1][20]\t Batch [350][550]\t Training Loss 0.2112\t Accuracy 0.8398\n",
      "Epoch [1][20]\t Batch [400][550]\t Training Loss 0.2092\t Accuracy 0.8420\n",
      "Epoch [1][20]\t Batch [450][550]\t Training Loss 0.2075\t Accuracy 0.8436\n",
      "Epoch [1][20]\t Batch [500][550]\t Training Loss 0.2058\t Accuracy 0.8447\n",
      "\n",
      "Epoch [1]\t Average training loss 0.2037\t Average training accuracy 0.8465\n",
      "Epoch [1]\t Average validation loss 0.1635\t Average validation accuracy 0.8970\n",
      "\n",
      "Epoch [2][20]\t Batch [0][550]\t Training Loss 0.1617\t Accuracy 0.9000\n",
      "Epoch [2][20]\t Batch [50][550]\t Training Loss 0.1753\t Accuracy 0.8765\n",
      "Epoch [2][20]\t Batch [100][550]\t Training Loss 0.1752\t Accuracy 0.8715\n",
      "Epoch [2][20]\t Batch [150][550]\t Training Loss 0.1781\t Accuracy 0.8670\n",
      "Epoch [2][20]\t Batch [200][550]\t Training Loss 0.1770\t Accuracy 0.8687\n",
      "Epoch [2][20]\t Batch [250][550]\t Training Loss 0.1757\t Accuracy 0.8686\n",
      "Epoch [2][20]\t Batch [300][550]\t Training Loss 0.1746\t Accuracy 0.8698\n",
      "Epoch [2][20]\t Batch [350][550]\t Training Loss 0.1742\t Accuracy 0.8696\n",
      "Epoch [2][20]\t Batch [400][550]\t Training Loss 0.1732\t Accuracy 0.8706\n",
      "Epoch [2][20]\t Batch [450][550]\t Training Loss 0.1723\t Accuracy 0.8710\n",
      "Epoch [2][20]\t Batch [500][550]\t Training Loss 0.1715\t Accuracy 0.8712\n",
      "\n",
      "Epoch [2]\t Average training loss 0.1702\t Average training accuracy 0.8721\n",
      "Epoch [2]\t Average validation loss 0.1384\t Average validation accuracy 0.9088\n",
      "\n",
      "Epoch [3][20]\t Batch [0][550]\t Training Loss 0.1359\t Accuracy 0.9200\n",
      "Epoch [3][20]\t Batch [50][550]\t Training Loss 0.1498\t Accuracy 0.8925\n",
      "Epoch [3][20]\t Batch [100][550]\t Training Loss 0.1510\t Accuracy 0.8875\n",
      "Epoch [3][20]\t Batch [150][550]\t Training Loss 0.1541\t Accuracy 0.8823\n",
      "Epoch [3][20]\t Batch [200][550]\t Training Loss 0.1534\t Accuracy 0.8839\n",
      "Epoch [3][20]\t Batch [250][550]\t Training Loss 0.1526\t Accuracy 0.8845\n",
      "Epoch [3][20]\t Batch [300][550]\t Training Loss 0.1519\t Accuracy 0.8850\n",
      "Epoch [3][20]\t Batch [350][550]\t Training Loss 0.1520\t Accuracy 0.8840\n",
      "Epoch [3][20]\t Batch [400][550]\t Training Loss 0.1514\t Accuracy 0.8842\n",
      "Epoch [3][20]\t Batch [450][550]\t Training Loss 0.1509\t Accuracy 0.8846\n",
      "Epoch [3][20]\t Batch [500][550]\t Training Loss 0.1506\t Accuracy 0.8845\n",
      "\n",
      "Epoch [3]\t Average training loss 0.1496\t Average training accuracy 0.8850\n",
      "Epoch [3]\t Average validation loss 0.1228\t Average validation accuracy 0.9198\n",
      "\n",
      "Epoch [4][20]\t Batch [0][550]\t Training Loss 0.1198\t Accuracy 0.9300\n",
      "Epoch [4][20]\t Batch [50][550]\t Training Loss 0.1337\t Accuracy 0.9018\n",
      "Epoch [4][20]\t Batch [100][550]\t Training Loss 0.1355\t Accuracy 0.8970\n",
      "Epoch [4][20]\t Batch [150][550]\t Training Loss 0.1386\t Accuracy 0.8925\n",
      "Epoch [4][20]\t Batch [200][550]\t Training Loss 0.1379\t Accuracy 0.8940\n",
      "Epoch [4][20]\t Batch [250][550]\t Training Loss 0.1375\t Accuracy 0.8945\n",
      "Epoch [4][20]\t Batch [300][550]\t Training Loss 0.1371\t Accuracy 0.8942\n",
      "Epoch [4][20]\t Batch [350][550]\t Training Loss 0.1374\t Accuracy 0.8932\n",
      "Epoch [4][20]\t Batch [400][550]\t Training Loss 0.1371\t Accuracy 0.8930\n",
      "Epoch [4][20]\t Batch [450][550]\t Training Loss 0.1368\t Accuracy 0.8933\n",
      "Epoch [4][20]\t Batch [500][550]\t Training Loss 0.1367\t Accuracy 0.8930\n",
      "\n",
      "Epoch [4]\t Average training loss 0.1360\t Average training accuracy 0.8935\n",
      "Epoch [4]\t Average validation loss 0.1125\t Average validation accuracy 0.9242\n",
      "\n",
      "Epoch [5][20]\t Batch [0][550]\t Training Loss 0.1094\t Accuracy 0.9400\n",
      "Epoch [5][20]\t Batch [50][550]\t Training Loss 0.1229\t Accuracy 0.9090\n",
      "Epoch [5][20]\t Batch [100][550]\t Training Loss 0.1249\t Accuracy 0.9041\n",
      "Epoch [5][20]\t Batch [150][550]\t Training Loss 0.1280\t Accuracy 0.8998\n",
      "Epoch [5][20]\t Batch [200][550]\t Training Loss 0.1273\t Accuracy 0.9015\n",
      "Epoch [5][20]\t Batch [250][550]\t Training Loss 0.1271\t Accuracy 0.9015\n",
      "Epoch [5][20]\t Batch [300][550]\t Training Loss 0.1269\t Accuracy 0.9012\n",
      "Epoch [5][20]\t Batch [350][550]\t Training Loss 0.1273\t Accuracy 0.8999\n",
      "Epoch [5][20]\t Batch [400][550]\t Training Loss 0.1271\t Accuracy 0.8997\n",
      "Epoch [5][20]\t Batch [450][550]\t Training Loss 0.1270\t Accuracy 0.8999\n",
      "Epoch [5][20]\t Batch [500][550]\t Training Loss 0.1270\t Accuracy 0.8995\n",
      "\n",
      "Epoch [5]\t Average training loss 0.1265\t Average training accuracy 0.8997\n",
      "Epoch [5]\t Average validation loss 0.1052\t Average validation accuracy 0.9286\n",
      "\n",
      "Epoch [6][20]\t Batch [0][550]\t Training Loss 0.1023\t Accuracy 0.9500\n",
      "Epoch [6][20]\t Batch [50][550]\t Training Loss 0.1153\t Accuracy 0.9155\n",
      "Epoch [6][20]\t Batch [100][550]\t Training Loss 0.1173\t Accuracy 0.9099\n",
      "Epoch [6][20]\t Batch [150][550]\t Training Loss 0.1203\t Accuracy 0.9055\n",
      "Epoch [6][20]\t Batch [200][550]\t Training Loss 0.1197\t Accuracy 0.9072\n",
      "Epoch [6][20]\t Batch [250][550]\t Training Loss 0.1195\t Accuracy 0.9071\n",
      "Epoch [6][20]\t Batch [300][550]\t Training Loss 0.1195\t Accuracy 0.9066\n",
      "Epoch [6][20]\t Batch [350][550]\t Training Loss 0.1199\t Accuracy 0.9054\n",
      "Epoch [6][20]\t Batch [400][550]\t Training Loss 0.1198\t Accuracy 0.9052\n",
      "Epoch [6][20]\t Batch [450][550]\t Training Loss 0.1198\t Accuracy 0.9053\n",
      "Epoch [6][20]\t Batch [500][550]\t Training Loss 0.1199\t Accuracy 0.9048\n",
      "\n",
      "Epoch [6]\t Average training loss 0.1196\t Average training accuracy 0.9049\n",
      "Epoch [6]\t Average validation loss 0.0999\t Average validation accuracy 0.9316\n",
      "\n",
      "Epoch [7][20]\t Batch [0][550]\t Training Loss 0.0972\t Accuracy 0.9500\n",
      "Epoch [7][20]\t Batch [50][550]\t Training Loss 0.1096\t Accuracy 0.9180\n",
      "Epoch [7][20]\t Batch [100][550]\t Training Loss 0.1115\t Accuracy 0.9129\n",
      "Epoch [7][20]\t Batch [150][550]\t Training Loss 0.1145\t Accuracy 0.9089\n",
      "Epoch [7][20]\t Batch [200][550]\t Training Loss 0.1139\t Accuracy 0.9105\n",
      "Epoch [7][20]\t Batch [250][550]\t Training Loss 0.1139\t Accuracy 0.9105\n",
      "Epoch [7][20]\t Batch [300][550]\t Training Loss 0.1139\t Accuracy 0.9099\n",
      "Epoch [7][20]\t Batch [350][550]\t Training Loss 0.1144\t Accuracy 0.9090\n",
      "Epoch [7][20]\t Batch [400][550]\t Training Loss 0.1143\t Accuracy 0.9088\n",
      "Epoch [7][20]\t Batch [450][550]\t Training Loss 0.1144\t Accuracy 0.9089\n",
      "Epoch [7][20]\t Batch [500][550]\t Training Loss 0.1146\t Accuracy 0.9083\n",
      "\n",
      "Epoch [7]\t Average training loss 0.1143\t Average training accuracy 0.9084\n",
      "Epoch [7]\t Average validation loss 0.0957\t Average validation accuracy 0.9336\n",
      "\n",
      "Epoch [8][20]\t Batch [0][550]\t Training Loss 0.0936\t Accuracy 0.9500\n",
      "Epoch [8][20]\t Batch [50][550]\t Training Loss 0.1052\t Accuracy 0.9194\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8][20]\t Batch [100][550]\t Training Loss 0.1071\t Accuracy 0.9151\n",
      "Epoch [8][20]\t Batch [150][550]\t Training Loss 0.1101\t Accuracy 0.9113\n",
      "Epoch [8][20]\t Batch [200][550]\t Training Loss 0.1095\t Accuracy 0.9129\n",
      "Epoch [8][20]\t Batch [250][550]\t Training Loss 0.1095\t Accuracy 0.9129\n",
      "Epoch [8][20]\t Batch [300][550]\t Training Loss 0.1096\t Accuracy 0.9125\n",
      "Epoch [8][20]\t Batch [350][550]\t Training Loss 0.1101\t Accuracy 0.9119\n",
      "Epoch [8][20]\t Batch [400][550]\t Training Loss 0.1101\t Accuracy 0.9117\n",
      "Epoch [8][20]\t Batch [450][550]\t Training Loss 0.1102\t Accuracy 0.9118\n",
      "Epoch [8][20]\t Batch [500][550]\t Training Loss 0.1105\t Accuracy 0.9113\n",
      "\n",
      "Epoch [8]\t Average training loss 0.1102\t Average training accuracy 0.9114\n",
      "Epoch [8]\t Average validation loss 0.0925\t Average validation accuracy 0.9360\n",
      "\n",
      "Epoch [9][20]\t Batch [0][550]\t Training Loss 0.0907\t Accuracy 0.9500\n",
      "Epoch [9][20]\t Batch [50][550]\t Training Loss 0.1018\t Accuracy 0.9208\n",
      "Epoch [9][20]\t Batch [100][550]\t Training Loss 0.1036\t Accuracy 0.9166\n",
      "Epoch [9][20]\t Batch [150][550]\t Training Loss 0.1066\t Accuracy 0.9130\n",
      "Epoch [9][20]\t Batch [200][550]\t Training Loss 0.1060\t Accuracy 0.9147\n",
      "Epoch [9][20]\t Batch [250][550]\t Training Loss 0.1060\t Accuracy 0.9152\n",
      "Epoch [9][20]\t Batch [300][550]\t Training Loss 0.1062\t Accuracy 0.9148\n",
      "Epoch [9][20]\t Batch [350][550]\t Training Loss 0.1067\t Accuracy 0.9142\n",
      "Epoch [9][20]\t Batch [400][550]\t Training Loss 0.1067\t Accuracy 0.9140\n",
      "Epoch [9][20]\t Batch [450][550]\t Training Loss 0.1069\t Accuracy 0.9140\n",
      "Epoch [9][20]\t Batch [500][550]\t Training Loss 0.1072\t Accuracy 0.9135\n",
      "\n",
      "Epoch [9]\t Average training loss 0.1069\t Average training accuracy 0.9135\n",
      "Epoch [9]\t Average validation loss 0.0900\t Average validation accuracy 0.9382\n",
      "\n",
      "Epoch [10][20]\t Batch [0][550]\t Training Loss 0.0885\t Accuracy 0.9500\n",
      "Epoch [10][20]\t Batch [50][550]\t Training Loss 0.0990\t Accuracy 0.9227\n",
      "Epoch [10][20]\t Batch [100][550]\t Training Loss 0.1008\t Accuracy 0.9189\n",
      "Epoch [10][20]\t Batch [150][550]\t Training Loss 0.1037\t Accuracy 0.9149\n",
      "Epoch [10][20]\t Batch [200][550]\t Training Loss 0.1031\t Accuracy 0.9166\n",
      "Epoch [10][20]\t Batch [250][550]\t Training Loss 0.1032\t Accuracy 0.9169\n",
      "Epoch [10][20]\t Batch [300][550]\t Training Loss 0.1034\t Accuracy 0.9164\n",
      "Epoch [10][20]\t Batch [350][550]\t Training Loss 0.1039\t Accuracy 0.9160\n",
      "Epoch [10][20]\t Batch [400][550]\t Training Loss 0.1040\t Accuracy 0.9158\n",
      "Epoch [10][20]\t Batch [450][550]\t Training Loss 0.1042\t Accuracy 0.9159\n",
      "Epoch [10][20]\t Batch [500][550]\t Training Loss 0.1045\t Accuracy 0.9152\n",
      "\n",
      "Epoch [10]\t Average training loss 0.1043\t Average training accuracy 0.9152\n",
      "Epoch [10]\t Average validation loss 0.0879\t Average validation accuracy 0.9384\n",
      "\n",
      "Epoch [11][20]\t Batch [0][550]\t Training Loss 0.0867\t Accuracy 0.9500\n",
      "Epoch [11][20]\t Batch [50][550]\t Training Loss 0.0967\t Accuracy 0.9233\n",
      "Epoch [11][20]\t Batch [100][550]\t Training Loss 0.0985\t Accuracy 0.9196\n",
      "Epoch [11][20]\t Batch [150][550]\t Training Loss 0.1014\t Accuracy 0.9160\n",
      "Epoch [11][20]\t Batch [200][550]\t Training Loss 0.1008\t Accuracy 0.9177\n",
      "Epoch [11][20]\t Batch [250][550]\t Training Loss 0.1009\t Accuracy 0.9181\n",
      "Epoch [11][20]\t Batch [300][550]\t Training Loss 0.1011\t Accuracy 0.9174\n",
      "Epoch [11][20]\t Batch [350][550]\t Training Loss 0.1017\t Accuracy 0.9171\n",
      "Epoch [11][20]\t Batch [400][550]\t Training Loss 0.1017\t Accuracy 0.9169\n",
      "Epoch [11][20]\t Batch [450][550]\t Training Loss 0.1019\t Accuracy 0.9170\n",
      "Epoch [11][20]\t Batch [500][550]\t Training Loss 0.1023\t Accuracy 0.9164\n",
      "\n",
      "Epoch [11]\t Average training loss 0.1021\t Average training accuracy 0.9164\n",
      "Epoch [11]\t Average validation loss 0.0861\t Average validation accuracy 0.9394\n",
      "\n",
      "Epoch [12][20]\t Batch [0][550]\t Training Loss 0.0852\t Accuracy 0.9500\n",
      "Epoch [12][20]\t Batch [50][550]\t Training Loss 0.0948\t Accuracy 0.9253\n",
      "Epoch [12][20]\t Batch [100][550]\t Training Loss 0.0965\t Accuracy 0.9213\n",
      "Epoch [12][20]\t Batch [150][550]\t Training Loss 0.0995\t Accuracy 0.9176\n",
      "Epoch [12][20]\t Batch [200][550]\t Training Loss 0.0989\t Accuracy 0.9192\n",
      "Epoch [12][20]\t Batch [250][550]\t Training Loss 0.0990\t Accuracy 0.9194\n",
      "Epoch [12][20]\t Batch [300][550]\t Training Loss 0.0992\t Accuracy 0.9187\n",
      "Epoch [12][20]\t Batch [350][550]\t Training Loss 0.0998\t Accuracy 0.9185\n",
      "Epoch [12][20]\t Batch [400][550]\t Training Loss 0.0998\t Accuracy 0.9182\n",
      "Epoch [12][20]\t Batch [450][550]\t Training Loss 0.1000\t Accuracy 0.9182\n",
      "Epoch [12][20]\t Batch [500][550]\t Training Loss 0.1004\t Accuracy 0.9177\n",
      "\n",
      "Epoch [12]\t Average training loss 0.1003\t Average training accuracy 0.9176\n",
      "Epoch [12]\t Average validation loss 0.0847\t Average validation accuracy 0.9408\n",
      "\n",
      "Epoch [13][20]\t Batch [0][550]\t Training Loss 0.0839\t Accuracy 0.9500\n",
      "Epoch [13][20]\t Batch [50][550]\t Training Loss 0.0932\t Accuracy 0.9265\n",
      "Epoch [13][20]\t Batch [100][550]\t Training Loss 0.0949\t Accuracy 0.9227\n",
      "Epoch [13][20]\t Batch [150][550]\t Training Loss 0.0978\t Accuracy 0.9191\n",
      "Epoch [13][20]\t Batch [200][550]\t Training Loss 0.0973\t Accuracy 0.9203\n",
      "Epoch [13][20]\t Batch [250][550]\t Training Loss 0.0974\t Accuracy 0.9205\n",
      "Epoch [13][20]\t Batch [300][550]\t Training Loss 0.0976\t Accuracy 0.9200\n",
      "Epoch [13][20]\t Batch [350][550]\t Training Loss 0.0981\t Accuracy 0.9198\n",
      "Epoch [13][20]\t Batch [400][550]\t Training Loss 0.0983\t Accuracy 0.9195\n",
      "Epoch [13][20]\t Batch [450][550]\t Training Loss 0.0984\t Accuracy 0.9195\n",
      "Epoch [13][20]\t Batch [500][550]\t Training Loss 0.0988\t Accuracy 0.9189\n",
      "\n",
      "Epoch [13]\t Average training loss 0.0987\t Average training accuracy 0.9189\n",
      "Epoch [13]\t Average validation loss 0.0835\t Average validation accuracy 0.9424\n",
      "\n",
      "Epoch [14][20]\t Batch [0][550]\t Training Loss 0.0828\t Accuracy 0.9500\n",
      "Epoch [14][20]\t Batch [50][550]\t Training Loss 0.0918\t Accuracy 0.9280\n",
      "Epoch [14][20]\t Batch [100][550]\t Training Loss 0.0935\t Accuracy 0.9239\n",
      "Epoch [14][20]\t Batch [150][550]\t Training Loss 0.0964\t Accuracy 0.9201\n",
      "Epoch [14][20]\t Batch [200][550]\t Training Loss 0.0959\t Accuracy 0.9211\n",
      "Epoch [14][20]\t Batch [250][550]\t Training Loss 0.0960\t Accuracy 0.9213\n",
      "Epoch [14][20]\t Batch [300][550]\t Training Loss 0.0962\t Accuracy 0.9208\n",
      "Epoch [14][20]\t Batch [350][550]\t Training Loss 0.0968\t Accuracy 0.9205\n",
      "Epoch [14][20]\t Batch [400][550]\t Training Loss 0.0969\t Accuracy 0.9202\n",
      "Epoch [14][20]\t Batch [450][550]\t Training Loss 0.0971\t Accuracy 0.9203\n",
      "Epoch [14][20]\t Batch [500][550]\t Training Loss 0.0975\t Accuracy 0.9197\n",
      "\n",
      "Epoch [14]\t Average training loss 0.0974\t Average training accuracy 0.9196\n",
      "Epoch [14]\t Average validation loss 0.0824\t Average validation accuracy 0.9434\n",
      "\n",
      "Epoch [15][20]\t Batch [0][550]\t Training Loss 0.0818\t Accuracy 0.9500\n",
      "Epoch [15][20]\t Batch [50][550]\t Training Loss 0.0906\t Accuracy 0.9288\n",
      "Epoch [15][20]\t Batch [100][550]\t Training Loss 0.0923\t Accuracy 0.9246\n",
      "Epoch [15][20]\t Batch [150][550]\t Training Loss 0.0952\t Accuracy 0.9207\n",
      "Epoch [15][20]\t Batch [200][550]\t Training Loss 0.0947\t Accuracy 0.9218\n",
      "Epoch [15][20]\t Batch [250][550]\t Training Loss 0.0948\t Accuracy 0.9219\n",
      "Epoch [15][20]\t Batch [300][550]\t Training Loss 0.0950\t Accuracy 0.9215\n",
      "Epoch [15][20]\t Batch [350][550]\t Training Loss 0.0956\t Accuracy 0.9212\n",
      "Epoch [15][20]\t Batch [400][550]\t Training Loss 0.0957\t Accuracy 0.9209\n",
      "Epoch [15][20]\t Batch [450][550]\t Training Loss 0.0959\t Accuracy 0.9210\n",
      "Epoch [15][20]\t Batch [500][550]\t Training Loss 0.0963\t Accuracy 0.9204\n",
      "\n",
      "Epoch [15]\t Average training loss 0.0962\t Average training accuracy 0.9203\n",
      "Epoch [15]\t Average validation loss 0.0815\t Average validation accuracy 0.9440\n",
      "\n",
      "Epoch [16][20]\t Batch [0][550]\t Training Loss 0.0810\t Accuracy 0.9500\n",
      "Epoch [16][20]\t Batch [50][550]\t Training Loss 0.0895\t Accuracy 0.9300\n",
      "Epoch [16][20]\t Batch [100][550]\t Training Loss 0.0913\t Accuracy 0.9260\n",
      "Epoch [16][20]\t Batch [150][550]\t Training Loss 0.0942\t Accuracy 0.9220\n",
      "Epoch [16][20]\t Batch [200][550]\t Training Loss 0.0936\t Accuracy 0.9230\n",
      "Epoch [16][20]\t Batch [250][550]\t Training Loss 0.0937\t Accuracy 0.9229\n",
      "Epoch [16][20]\t Batch [300][550]\t Training Loss 0.0940\t Accuracy 0.9226\n",
      "Epoch [16][20]\t Batch [350][550]\t Training Loss 0.0945\t Accuracy 0.9223\n",
      "Epoch [16][20]\t Batch [400][550]\t Training Loss 0.0947\t Accuracy 0.9219\n",
      "Epoch [16][20]\t Batch [450][550]\t Training Loss 0.0949\t Accuracy 0.9219\n",
      "Epoch [16][20]\t Batch [500][550]\t Training Loss 0.0953\t Accuracy 0.9213\n",
      "\n",
      "Epoch [16]\t Average training loss 0.0952\t Average training accuracy 0.9212\n",
      "Epoch [16]\t Average validation loss 0.0808\t Average validation accuracy 0.9444\n",
      "\n",
      "Epoch [17][20]\t Batch [0][550]\t Training Loss 0.0803\t Accuracy 0.9500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17][20]\t Batch [50][550]\t Training Loss 0.0887\t Accuracy 0.9298\n",
      "Epoch [17][20]\t Batch [100][550]\t Training Loss 0.0904\t Accuracy 0.9263\n",
      "Epoch [17][20]\t Batch [150][550]\t Training Loss 0.0933\t Accuracy 0.9223\n",
      "Epoch [17][20]\t Batch [200][550]\t Training Loss 0.0927\t Accuracy 0.9235\n",
      "Epoch [17][20]\t Batch [250][550]\t Training Loss 0.0928\t Accuracy 0.9234\n",
      "Epoch [17][20]\t Batch [300][550]\t Training Loss 0.0931\t Accuracy 0.9231\n",
      "Epoch [17][20]\t Batch [350][550]\t Training Loss 0.0936\t Accuracy 0.9227\n",
      "Epoch [17][20]\t Batch [400][550]\t Training Loss 0.0938\t Accuracy 0.9225\n",
      "Epoch [17][20]\t Batch [450][550]\t Training Loss 0.0940\t Accuracy 0.9225\n",
      "Epoch [17][20]\t Batch [500][550]\t Training Loss 0.0944\t Accuracy 0.9219\n",
      "\n",
      "Epoch [17]\t Average training loss 0.0943\t Average training accuracy 0.9217\n",
      "Epoch [17]\t Average validation loss 0.0801\t Average validation accuracy 0.9446\n",
      "\n",
      "Epoch [18][20]\t Batch [0][550]\t Training Loss 0.0797\t Accuracy 0.9500\n",
      "Epoch [18][20]\t Batch [50][550]\t Training Loss 0.0879\t Accuracy 0.9296\n",
      "Epoch [18][20]\t Batch [100][550]\t Training Loss 0.0896\t Accuracy 0.9262\n",
      "Epoch [18][20]\t Batch [150][550]\t Training Loss 0.0925\t Accuracy 0.9225\n",
      "Epoch [18][20]\t Batch [200][550]\t Training Loss 0.0919\t Accuracy 0.9238\n",
      "Epoch [18][20]\t Batch [250][550]\t Training Loss 0.0920\t Accuracy 0.9237\n",
      "Epoch [18][20]\t Batch [300][550]\t Training Loss 0.0923\t Accuracy 0.9234\n",
      "Epoch [18][20]\t Batch [350][550]\t Training Loss 0.0929\t Accuracy 0.9231\n",
      "Epoch [18][20]\t Batch [400][550]\t Training Loss 0.0930\t Accuracy 0.9229\n",
      "Epoch [18][20]\t Batch [450][550]\t Training Loss 0.0932\t Accuracy 0.9229\n",
      "Epoch [18][20]\t Batch [500][550]\t Training Loss 0.0937\t Accuracy 0.9223\n",
      "\n",
      "Epoch [18]\t Average training loss 0.0936\t Average training accuracy 0.9222\n",
      "Epoch [18]\t Average validation loss 0.0796\t Average validation accuracy 0.9450\n",
      "\n",
      "Epoch [19][20]\t Batch [0][550]\t Training Loss 0.0791\t Accuracy 0.9500\n",
      "Epoch [19][20]\t Batch [50][550]\t Training Loss 0.0872\t Accuracy 0.9300\n",
      "Epoch [19][20]\t Batch [100][550]\t Training Loss 0.0889\t Accuracy 0.9267\n",
      "Epoch [19][20]\t Batch [150][550]\t Training Loss 0.0918\t Accuracy 0.9229\n",
      "Epoch [19][20]\t Batch [200][550]\t Training Loss 0.0912\t Accuracy 0.9244\n",
      "Epoch [19][20]\t Batch [250][550]\t Training Loss 0.0913\t Accuracy 0.9243\n",
      "Epoch [19][20]\t Batch [300][550]\t Training Loss 0.0916\t Accuracy 0.9240\n",
      "Epoch [19][20]\t Batch [350][550]\t Training Loss 0.0922\t Accuracy 0.9236\n",
      "Epoch [19][20]\t Batch [400][550]\t Training Loss 0.0923\t Accuracy 0.9234\n",
      "Epoch [19][20]\t Batch [450][550]\t Training Loss 0.0925\t Accuracy 0.9234\n",
      "Epoch [19][20]\t Batch [500][550]\t Training Loss 0.0930\t Accuracy 0.9228\n",
      "\n",
      "Epoch [19]\t Average training loss 0.0929\t Average training accuracy 0.9227\n",
      "Epoch [19]\t Average validation loss 0.0791\t Average validation accuracy 0.9450\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sigmoidMLP, sigmoid_loss, sigmoid_acc = train(sigmoidMLP, criterion, sgd, data_train, max_epoch, batch_size, disp_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing...\n",
      "The test accuracy is 0.9278.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test(sigmoidMLP, criterion, data_test, batch_size, disp_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 MLP with Euclidean Loss and ReLU Activation Function\n",
    "Build and train a MLP contraining one hidden layer with 128 units using ReLU activation function and Euclidean loss function.\n",
    "\n",
    "### TODO\n",
    "Before executing the following code, you should complete **layers/relu_layer.py**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from layers import ReLULayer\n",
    "\n",
    "reluMLP = Network()\n",
    "# TODO build ReLUMLP with FCLayer and ReLULayer\n",
    "reluMLP.add(FCLayer(784, 128))\n",
    "reluMLP.add(ReLULayer())\n",
    "reluMLP.add(FCLayer(128, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0][20]\t Batch [0][550]\t Training Loss 3.6030\t Accuracy 0.1100\n",
      "Epoch [0][20]\t Batch [50][550]\t Training Loss 0.8152\t Accuracy 0.4373\n",
      "Epoch [0][20]\t Batch [100][550]\t Training Loss 0.5500\t Accuracy 0.5839\n",
      "Epoch [0][20]\t Batch [150][550]\t Training Loss 0.4464\t Accuracy 0.6500\n",
      "Epoch [0][20]\t Batch [200][550]\t Training Loss 0.3864\t Accuracy 0.6945\n",
      "Epoch [0][20]\t Batch [250][550]\t Training Loss 0.3453\t Accuracy 0.7263\n",
      "Epoch [0][20]\t Batch [300][550]\t Training Loss 0.3159\t Accuracy 0.7493\n",
      "Epoch [0][20]\t Batch [350][550]\t Training Loss 0.2934\t Accuracy 0.7664\n",
      "Epoch [0][20]\t Batch [400][550]\t Training Loss 0.2750\t Accuracy 0.7810\n",
      "Epoch [0][20]\t Batch [450][550]\t Training Loss 0.2601\t Accuracy 0.7929\n",
      "Epoch [0][20]\t Batch [500][550]\t Training Loss 0.2478\t Accuracy 0.8020\n",
      "\n",
      "Epoch [0]\t Average training loss 0.2368\t Average training accuracy 0.8106\n",
      "Epoch [0]\t Average validation loss 0.1078\t Average validation accuracy 0.9244\n",
      "\n",
      "Epoch [1][20]\t Batch [0][550]\t Training Loss 0.1085\t Accuracy 0.9400\n",
      "Epoch [1][20]\t Batch [50][550]\t Training Loss 0.1161\t Accuracy 0.9096\n",
      "Epoch [1][20]\t Batch [100][550]\t Training Loss 0.1166\t Accuracy 0.9061\n",
      "Epoch [1][20]\t Batch [150][550]\t Training Loss 0.1177\t Accuracy 0.9046\n",
      "Epoch [1][20]\t Batch [200][550]\t Training Loss 0.1159\t Accuracy 0.9058\n",
      "Epoch [1][20]\t Batch [250][550]\t Training Loss 0.1144\t Accuracy 0.9071\n",
      "Epoch [1][20]\t Batch [300][550]\t Training Loss 0.1136\t Accuracy 0.9073\n",
      "Epoch [1][20]\t Batch [350][550]\t Training Loss 0.1127\t Accuracy 0.9072\n",
      "Epoch [1][20]\t Batch [400][550]\t Training Loss 0.1119\t Accuracy 0.9082\n",
      "Epoch [1][20]\t Batch [450][550]\t Training Loss 0.1110\t Accuracy 0.9090\n",
      "Epoch [1][20]\t Batch [500][550]\t Training Loss 0.1105\t Accuracy 0.9090\n",
      "\n",
      "Epoch [1]\t Average training loss 0.1093\t Average training accuracy 0.9099\n",
      "Epoch [1]\t Average validation loss 0.0851\t Average validation accuracy 0.9398\n",
      "\n",
      "Epoch [2][20]\t Batch [0][550]\t Training Loss 0.0871\t Accuracy 0.9400\n",
      "Epoch [2][20]\t Batch [50][550]\t Training Loss 0.0926\t Accuracy 0.9280\n",
      "Epoch [2][20]\t Batch [100][550]\t Training Loss 0.0940\t Accuracy 0.9260\n",
      "Epoch [2][20]\t Batch [150][550]\t Training Loss 0.0957\t Accuracy 0.9241\n",
      "Epoch [2][20]\t Batch [200][550]\t Training Loss 0.0947\t Accuracy 0.9251\n",
      "Epoch [2][20]\t Batch [250][550]\t Training Loss 0.0941\t Accuracy 0.9246\n",
      "Epoch [2][20]\t Batch [300][550]\t Training Loss 0.0942\t Accuracy 0.9242\n",
      "Epoch [2][20]\t Batch [350][550]\t Training Loss 0.0940\t Accuracy 0.9238\n",
      "Epoch [2][20]\t Batch [400][550]\t Training Loss 0.0938\t Accuracy 0.9244\n",
      "Epoch [2][20]\t Batch [450][550]\t Training Loss 0.0935\t Accuracy 0.9246\n",
      "Epoch [2][20]\t Batch [500][550]\t Training Loss 0.0936\t Accuracy 0.9241\n",
      "\n",
      "Epoch [2]\t Average training loss 0.0930\t Average training accuracy 0.9244\n",
      "Epoch [2]\t Average validation loss 0.0764\t Average validation accuracy 0.9466\n",
      "\n",
      "Epoch [3][20]\t Batch [0][550]\t Training Loss 0.0756\t Accuracy 0.9500\n",
      "Epoch [3][20]\t Batch [50][550]\t Training Loss 0.0827\t Accuracy 0.9361\n",
      "Epoch [3][20]\t Batch [100][550]\t Training Loss 0.0843\t Accuracy 0.9348\n",
      "Epoch [3][20]\t Batch [150][550]\t Training Loss 0.0860\t Accuracy 0.9330\n",
      "Epoch [3][20]\t Batch [200][550]\t Training Loss 0.0853\t Accuracy 0.9342\n",
      "Epoch [3][20]\t Batch [250][550]\t Training Loss 0.0849\t Accuracy 0.9340\n",
      "Epoch [3][20]\t Batch [300][550]\t Training Loss 0.0853\t Accuracy 0.9334\n",
      "Epoch [3][20]\t Batch [350][550]\t Training Loss 0.0853\t Accuracy 0.9325\n",
      "Epoch [3][20]\t Batch [400][550]\t Training Loss 0.0853\t Accuracy 0.9326\n",
      "Epoch [3][20]\t Batch [450][550]\t Training Loss 0.0851\t Accuracy 0.9327\n",
      "Epoch [3][20]\t Batch [500][550]\t Training Loss 0.0854\t Accuracy 0.9321\n",
      "\n",
      "Epoch [3]\t Average training loss 0.0850\t Average training accuracy 0.9322\n",
      "Epoch [3]\t Average validation loss 0.0713\t Average validation accuracy 0.9508\n",
      "\n",
      "Epoch [4][20]\t Batch [0][550]\t Training Loss 0.0677\t Accuracy 0.9500\n",
      "Epoch [4][20]\t Batch [50][550]\t Training Loss 0.0767\t Accuracy 0.9418\n",
      "Epoch [4][20]\t Batch [100][550]\t Training Loss 0.0785\t Accuracy 0.9396\n",
      "Epoch [4][20]\t Batch [150][550]\t Training Loss 0.0802\t Accuracy 0.9386\n",
      "Epoch [4][20]\t Batch [200][550]\t Training Loss 0.0795\t Accuracy 0.9395\n",
      "Epoch [4][20]\t Batch [250][550]\t Training Loss 0.0793\t Accuracy 0.9388\n",
      "Epoch [4][20]\t Batch [300][550]\t Training Loss 0.0797\t Accuracy 0.9381\n",
      "Epoch [4][20]\t Batch [350][550]\t Training Loss 0.0798\t Accuracy 0.9376\n",
      "Epoch [4][20]\t Batch [400][550]\t Training Loss 0.0798\t Accuracy 0.9376\n",
      "Epoch [4][20]\t Batch [450][550]\t Training Loss 0.0798\t Accuracy 0.9376\n",
      "Epoch [4][20]\t Batch [500][550]\t Training Loss 0.0801\t Accuracy 0.9369\n",
      "\n",
      "Epoch [4]\t Average training loss 0.0797\t Average training accuracy 0.9369\n",
      "Epoch [4]\t Average validation loss 0.0679\t Average validation accuracy 0.9530\n",
      "\n",
      "Epoch [5][20]\t Batch [0][550]\t Training Loss 0.0621\t Accuracy 0.9700\n",
      "Epoch [5][20]\t Batch [50][550]\t Training Loss 0.0726\t Accuracy 0.9455\n",
      "Epoch [5][20]\t Batch [100][550]\t Training Loss 0.0744\t Accuracy 0.9441\n",
      "Epoch [5][20]\t Batch [150][550]\t Training Loss 0.0761\t Accuracy 0.9429\n",
      "Epoch [5][20]\t Batch [200][550]\t Training Loss 0.0754\t Accuracy 0.9440\n",
      "Epoch [5][20]\t Batch [250][550]\t Training Loss 0.0752\t Accuracy 0.9432\n",
      "Epoch [5][20]\t Batch [300][550]\t Training Loss 0.0758\t Accuracy 0.9425\n",
      "Epoch [5][20]\t Batch [350][550]\t Training Loss 0.0758\t Accuracy 0.9421\n",
      "Epoch [5][20]\t Batch [400][550]\t Training Loss 0.0759\t Accuracy 0.9419\n",
      "Epoch [5][20]\t Batch [450][550]\t Training Loss 0.0759\t Accuracy 0.9418\n",
      "Epoch [5][20]\t Batch [500][550]\t Training Loss 0.0762\t Accuracy 0.9412\n",
      "\n",
      "Epoch [5]\t Average training loss 0.0759\t Average training accuracy 0.9411\n",
      "Epoch [5]\t Average validation loss 0.0655\t Average validation accuracy 0.9544\n",
      "\n",
      "Epoch [6][20]\t Batch [0][550]\t Training Loss 0.0582\t Accuracy 0.9700\n",
      "Epoch [6][20]\t Batch [50][550]\t Training Loss 0.0695\t Accuracy 0.9498\n",
      "Epoch [6][20]\t Batch [100][550]\t Training Loss 0.0713\t Accuracy 0.9469\n",
      "Epoch [6][20]\t Batch [150][550]\t Training Loss 0.0729\t Accuracy 0.9459\n",
      "Epoch [6][20]\t Batch [200][550]\t Training Loss 0.0722\t Accuracy 0.9471\n",
      "Epoch [6][20]\t Batch [250][550]\t Training Loss 0.0721\t Accuracy 0.9466\n",
      "Epoch [6][20]\t Batch [300][550]\t Training Loss 0.0727\t Accuracy 0.9457\n",
      "Epoch [6][20]\t Batch [350][550]\t Training Loss 0.0727\t Accuracy 0.9453\n",
      "Epoch [6][20]\t Batch [400][550]\t Training Loss 0.0728\t Accuracy 0.9451\n",
      "Epoch [6][20]\t Batch [450][550]\t Training Loss 0.0728\t Accuracy 0.9449\n",
      "Epoch [6][20]\t Batch [500][550]\t Training Loss 0.0731\t Accuracy 0.9443\n",
      "\n",
      "Epoch [6]\t Average training loss 0.0729\t Average training accuracy 0.9443\n",
      "Epoch [6]\t Average validation loss 0.0637\t Average validation accuracy 0.9578\n",
      "\n",
      "Epoch [7][20]\t Batch [0][550]\t Training Loss 0.0559\t Accuracy 0.9700\n",
      "Epoch [7][20]\t Batch [50][550]\t Training Loss 0.0669\t Accuracy 0.9541\n",
      "Epoch [7][20]\t Batch [100][550]\t Training Loss 0.0688\t Accuracy 0.9495\n",
      "Epoch [7][20]\t Batch [150][550]\t Training Loss 0.0704\t Accuracy 0.9483\n",
      "Epoch [7][20]\t Batch [200][550]\t Training Loss 0.0697\t Accuracy 0.9494\n",
      "Epoch [7][20]\t Batch [250][550]\t Training Loss 0.0696\t Accuracy 0.9490\n",
      "Epoch [7][20]\t Batch [300][550]\t Training Loss 0.0701\t Accuracy 0.9484\n",
      "Epoch [7][20]\t Batch [350][550]\t Training Loss 0.0702\t Accuracy 0.9479\n",
      "Epoch [7][20]\t Batch [400][550]\t Training Loss 0.0703\t Accuracy 0.9477\n",
      "Epoch [7][20]\t Batch [450][550]\t Training Loss 0.0703\t Accuracy 0.9476\n",
      "Epoch [7][20]\t Batch [500][550]\t Training Loss 0.0706\t Accuracy 0.9469\n",
      "\n",
      "Epoch [7]\t Average training loss 0.0704\t Average training accuracy 0.9470\n",
      "Epoch [7]\t Average validation loss 0.0621\t Average validation accuracy 0.9582\n",
      "\n",
      "Epoch [8][20]\t Batch [0][550]\t Training Loss 0.0538\t Accuracy 0.9700\n",
      "Epoch [8][20]\t Batch [50][550]\t Training Loss 0.0648\t Accuracy 0.9561\n",
      "Epoch [8][20]\t Batch [100][550]\t Training Loss 0.0666\t Accuracy 0.9519\n",
      "Epoch [8][20]\t Batch [150][550]\t Training Loss 0.0683\t Accuracy 0.9509\n",
      "Epoch [8][20]\t Batch [200][550]\t Training Loss 0.0676\t Accuracy 0.9521\n",
      "Epoch [8][20]\t Batch [250][550]\t Training Loss 0.0675\t Accuracy 0.9516\n",
      "Epoch [8][20]\t Batch [300][550]\t Training Loss 0.0680\t Accuracy 0.9509\n",
      "Epoch [8][20]\t Batch [350][550]\t Training Loss 0.0680\t Accuracy 0.9504\n",
      "Epoch [8][20]\t Batch [400][550]\t Training Loss 0.0681\t Accuracy 0.9502\n",
      "Epoch [8][20]\t Batch [450][550]\t Training Loss 0.0681\t Accuracy 0.9503\n",
      "Epoch [8][20]\t Batch [500][550]\t Training Loss 0.0685\t Accuracy 0.9495\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [8]\t Average training loss 0.0683\t Average training accuracy 0.9495\n",
      "Epoch [8]\t Average validation loss 0.0607\t Average validation accuracy 0.9590\n",
      "\n",
      "Epoch [9][20]\t Batch [0][550]\t Training Loss 0.0524\t Accuracy 0.9700\n",
      "Epoch [9][20]\t Batch [50][550]\t Training Loss 0.0630\t Accuracy 0.9569\n",
      "Epoch [9][20]\t Batch [100][550]\t Training Loss 0.0648\t Accuracy 0.9528\n",
      "Epoch [9][20]\t Batch [150][550]\t Training Loss 0.0664\t Accuracy 0.9522\n",
      "Epoch [9][20]\t Batch [200][550]\t Training Loss 0.0658\t Accuracy 0.9532\n",
      "Epoch [9][20]\t Batch [250][550]\t Training Loss 0.0657\t Accuracy 0.9530\n",
      "Epoch [9][20]\t Batch [300][550]\t Training Loss 0.0662\t Accuracy 0.9521\n",
      "Epoch [9][20]\t Batch [350][550]\t Training Loss 0.0662\t Accuracy 0.9518\n",
      "Epoch [9][20]\t Batch [400][550]\t Training Loss 0.0663\t Accuracy 0.9516\n",
      "Epoch [9][20]\t Batch [450][550]\t Training Loss 0.0663\t Accuracy 0.9517\n",
      "Epoch [9][20]\t Batch [500][550]\t Training Loss 0.0667\t Accuracy 0.9510\n",
      "\n",
      "Epoch [9]\t Average training loss 0.0664\t Average training accuracy 0.9510\n",
      "Epoch [9]\t Average validation loss 0.0595\t Average validation accuracy 0.9612\n",
      "\n",
      "Epoch [10][20]\t Batch [0][550]\t Training Loss 0.0511\t Accuracy 0.9700\n",
      "Epoch [10][20]\t Batch [50][550]\t Training Loss 0.0614\t Accuracy 0.9578\n",
      "Epoch [10][20]\t Batch [100][550]\t Training Loss 0.0632\t Accuracy 0.9547\n",
      "Epoch [10][20]\t Batch [150][550]\t Training Loss 0.0648\t Accuracy 0.9537\n",
      "Epoch [10][20]\t Batch [200][550]\t Training Loss 0.0642\t Accuracy 0.9548\n",
      "Epoch [10][20]\t Batch [250][550]\t Training Loss 0.0641\t Accuracy 0.9548\n",
      "Epoch [10][20]\t Batch [300][550]\t Training Loss 0.0646\t Accuracy 0.9541\n",
      "Epoch [10][20]\t Batch [350][550]\t Training Loss 0.0645\t Accuracy 0.9536\n",
      "Epoch [10][20]\t Batch [400][550]\t Training Loss 0.0646\t Accuracy 0.9534\n",
      "Epoch [10][20]\t Batch [450][550]\t Training Loss 0.0647\t Accuracy 0.9535\n",
      "Epoch [10][20]\t Batch [500][550]\t Training Loss 0.0650\t Accuracy 0.9528\n",
      "\n",
      "Epoch [10]\t Average training loss 0.0648\t Average training accuracy 0.9528\n",
      "Epoch [10]\t Average validation loss 0.0583\t Average validation accuracy 0.9624\n",
      "\n",
      "Epoch [11][20]\t Batch [0][550]\t Training Loss 0.0502\t Accuracy 0.9700\n",
      "Epoch [11][20]\t Batch [50][550]\t Training Loss 0.0600\t Accuracy 0.9600\n",
      "Epoch [11][20]\t Batch [100][550]\t Training Loss 0.0619\t Accuracy 0.9562\n",
      "Epoch [11][20]\t Batch [150][550]\t Training Loss 0.0634\t Accuracy 0.9553\n",
      "Epoch [11][20]\t Batch [200][550]\t Training Loss 0.0628\t Accuracy 0.9565\n",
      "Epoch [11][20]\t Batch [250][550]\t Training Loss 0.0626\t Accuracy 0.9565\n",
      "Epoch [11][20]\t Batch [300][550]\t Training Loss 0.0631\t Accuracy 0.9555\n",
      "Epoch [11][20]\t Batch [350][550]\t Training Loss 0.0631\t Accuracy 0.9552\n",
      "Epoch [11][20]\t Batch [400][550]\t Training Loss 0.0632\t Accuracy 0.9552\n",
      "Epoch [11][20]\t Batch [450][550]\t Training Loss 0.0632\t Accuracy 0.9553\n",
      "Epoch [11][20]\t Batch [500][550]\t Training Loss 0.0636\t Accuracy 0.9546\n",
      "\n",
      "Epoch [11]\t Average training loss 0.0634\t Average training accuracy 0.9547\n",
      "Epoch [11]\t Average validation loss 0.0573\t Average validation accuracy 0.9628\n",
      "\n",
      "Epoch [12][20]\t Batch [0][550]\t Training Loss 0.0492\t Accuracy 0.9700\n",
      "Epoch [12][20]\t Batch [50][550]\t Training Loss 0.0588\t Accuracy 0.9612\n",
      "Epoch [12][20]\t Batch [100][550]\t Training Loss 0.0606\t Accuracy 0.9578\n",
      "Epoch [12][20]\t Batch [150][550]\t Training Loss 0.0620\t Accuracy 0.9571\n",
      "Epoch [12][20]\t Batch [200][550]\t Training Loss 0.0614\t Accuracy 0.9581\n",
      "Epoch [12][20]\t Batch [250][550]\t Training Loss 0.0612\t Accuracy 0.9581\n",
      "Epoch [12][20]\t Batch [300][550]\t Training Loss 0.0617\t Accuracy 0.9570\n",
      "Epoch [12][20]\t Batch [350][550]\t Training Loss 0.0617\t Accuracy 0.9571\n",
      "Epoch [12][20]\t Batch [400][550]\t Training Loss 0.0618\t Accuracy 0.9569\n",
      "Epoch [12][20]\t Batch [450][550]\t Training Loss 0.0618\t Accuracy 0.9569\n",
      "Epoch [12][20]\t Batch [500][550]\t Training Loss 0.0622\t Accuracy 0.9563\n",
      "\n",
      "Epoch [12]\t Average training loss 0.0620\t Average training accuracy 0.9564\n",
      "Epoch [12]\t Average validation loss 0.0561\t Average validation accuracy 0.9642\n",
      "\n",
      "Epoch [13][20]\t Batch [0][550]\t Training Loss 0.0485\t Accuracy 0.9700\n",
      "Epoch [13][20]\t Batch [50][550]\t Training Loss 0.0576\t Accuracy 0.9618\n",
      "Epoch [13][20]\t Batch [100][550]\t Training Loss 0.0594\t Accuracy 0.9585\n",
      "Epoch [13][20]\t Batch [150][550]\t Training Loss 0.0607\t Accuracy 0.9578\n",
      "Epoch [13][20]\t Batch [200][550]\t Training Loss 0.0601\t Accuracy 0.9590\n",
      "Epoch [13][20]\t Batch [250][550]\t Training Loss 0.0599\t Accuracy 0.9589\n",
      "Epoch [13][20]\t Batch [300][550]\t Training Loss 0.0605\t Accuracy 0.9580\n",
      "Epoch [13][20]\t Batch [350][550]\t Training Loss 0.0604\t Accuracy 0.9580\n",
      "Epoch [13][20]\t Batch [400][550]\t Training Loss 0.0605\t Accuracy 0.9580\n",
      "Epoch [13][20]\t Batch [450][550]\t Training Loss 0.0605\t Accuracy 0.9579\n",
      "Epoch [13][20]\t Batch [500][550]\t Training Loss 0.0609\t Accuracy 0.9574\n",
      "\n",
      "Epoch [13]\t Average training loss 0.0607\t Average training accuracy 0.9574\n",
      "Epoch [13]\t Average validation loss 0.0553\t Average validation accuracy 0.9658\n",
      "\n",
      "Epoch [14][20]\t Batch [0][550]\t Training Loss 0.0480\t Accuracy 0.9700\n",
      "Epoch [14][20]\t Batch [50][550]\t Training Loss 0.0566\t Accuracy 0.9629\n",
      "Epoch [14][20]\t Batch [100][550]\t Training Loss 0.0583\t Accuracy 0.9597\n",
      "Epoch [14][20]\t Batch [150][550]\t Training Loss 0.0596\t Accuracy 0.9593\n",
      "Epoch [14][20]\t Batch [200][550]\t Training Loss 0.0590\t Accuracy 0.9604\n",
      "Epoch [14][20]\t Batch [250][550]\t Training Loss 0.0588\t Accuracy 0.9603\n",
      "Epoch [14][20]\t Batch [300][550]\t Training Loss 0.0593\t Accuracy 0.9595\n",
      "Epoch [14][20]\t Batch [350][550]\t Training Loss 0.0593\t Accuracy 0.9594\n",
      "Epoch [14][20]\t Batch [400][550]\t Training Loss 0.0594\t Accuracy 0.9595\n",
      "Epoch [14][20]\t Batch [450][550]\t Training Loss 0.0594\t Accuracy 0.9594\n",
      "Epoch [14][20]\t Batch [500][550]\t Training Loss 0.0598\t Accuracy 0.9588\n",
      "\n",
      "Epoch [14]\t Average training loss 0.0596\t Average training accuracy 0.9588\n",
      "Epoch [14]\t Average validation loss 0.0545\t Average validation accuracy 0.9664\n",
      "\n",
      "Epoch [15][20]\t Batch [0][550]\t Training Loss 0.0476\t Accuracy 0.9700\n",
      "Epoch [15][20]\t Batch [50][550]\t Training Loss 0.0556\t Accuracy 0.9639\n",
      "Epoch [15][20]\t Batch [100][550]\t Training Loss 0.0574\t Accuracy 0.9606\n",
      "Epoch [15][20]\t Batch [150][550]\t Training Loss 0.0585\t Accuracy 0.9602\n",
      "Epoch [15][20]\t Batch [200][550]\t Training Loss 0.0580\t Accuracy 0.9611\n",
      "Epoch [15][20]\t Batch [250][550]\t Training Loss 0.0578\t Accuracy 0.9610\n",
      "Epoch [15][20]\t Batch [300][550]\t Training Loss 0.0583\t Accuracy 0.9604\n",
      "Epoch [15][20]\t Batch [350][550]\t Training Loss 0.0583\t Accuracy 0.9604\n",
      "Epoch [15][20]\t Batch [400][550]\t Training Loss 0.0584\t Accuracy 0.9604\n",
      "Epoch [15][20]\t Batch [450][550]\t Training Loss 0.0584\t Accuracy 0.9604\n",
      "Epoch [15][20]\t Batch [500][550]\t Training Loss 0.0588\t Accuracy 0.9598\n",
      "\n",
      "Epoch [15]\t Average training loss 0.0586\t Average training accuracy 0.9598\n",
      "Epoch [15]\t Average validation loss 0.0538\t Average validation accuracy 0.9674\n",
      "\n",
      "Epoch [16][20]\t Batch [0][550]\t Training Loss 0.0471\t Accuracy 0.9700\n",
      "Epoch [16][20]\t Batch [50][550]\t Training Loss 0.0548\t Accuracy 0.9647\n",
      "Epoch [16][20]\t Batch [100][550]\t Training Loss 0.0565\t Accuracy 0.9613\n",
      "Epoch [16][20]\t Batch [150][550]\t Training Loss 0.0576\t Accuracy 0.9611\n",
      "Epoch [16][20]\t Batch [200][550]\t Training Loss 0.0571\t Accuracy 0.9618\n",
      "Epoch [16][20]\t Batch [250][550]\t Training Loss 0.0568\t Accuracy 0.9618\n",
      "Epoch [16][20]\t Batch [300][550]\t Training Loss 0.0574\t Accuracy 0.9611\n",
      "Epoch [16][20]\t Batch [350][550]\t Training Loss 0.0574\t Accuracy 0.9611\n",
      "Epoch [16][20]\t Batch [400][550]\t Training Loss 0.0575\t Accuracy 0.9612\n",
      "Epoch [16][20]\t Batch [450][550]\t Training Loss 0.0575\t Accuracy 0.9612\n",
      "Epoch [16][20]\t Batch [500][550]\t Training Loss 0.0579\t Accuracy 0.9606\n",
      "\n",
      "Epoch [16]\t Average training loss 0.0577\t Average training accuracy 0.9607\n",
      "Epoch [16]\t Average validation loss 0.0531\t Average validation accuracy 0.9680\n",
      "\n",
      "Epoch [17][20]\t Batch [0][550]\t Training Loss 0.0468\t Accuracy 0.9700\n",
      "Epoch [17][20]\t Batch [50][550]\t Training Loss 0.0540\t Accuracy 0.9647\n",
      "Epoch [17][20]\t Batch [100][550]\t Training Loss 0.0556\t Accuracy 0.9621\n",
      "Epoch [17][20]\t Batch [150][550]\t Training Loss 0.0567\t Accuracy 0.9616\n",
      "Epoch [17][20]\t Batch [200][550]\t Training Loss 0.0562\t Accuracy 0.9623\n",
      "Epoch [17][20]\t Batch [250][550]\t Training Loss 0.0559\t Accuracy 0.9625\n",
      "Epoch [17][20]\t Batch [300][550]\t Training Loss 0.0565\t Accuracy 0.9620\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17][20]\t Batch [350][550]\t Training Loss 0.0565\t Accuracy 0.9621\n",
      "Epoch [17][20]\t Batch [400][550]\t Training Loss 0.0566\t Accuracy 0.9621\n",
      "Epoch [17][20]\t Batch [450][550]\t Training Loss 0.0566\t Accuracy 0.9622\n",
      "Epoch [17][20]\t Batch [500][550]\t Training Loss 0.0570\t Accuracy 0.9617\n",
      "\n",
      "Epoch [17]\t Average training loss 0.0569\t Average training accuracy 0.9617\n",
      "Epoch [17]\t Average validation loss 0.0525\t Average validation accuracy 0.9682\n",
      "\n",
      "Epoch [18][20]\t Batch [0][550]\t Training Loss 0.0466\t Accuracy 0.9700\n",
      "Epoch [18][20]\t Batch [50][550]\t Training Loss 0.0532\t Accuracy 0.9659\n",
      "Epoch [18][20]\t Batch [100][550]\t Training Loss 0.0549\t Accuracy 0.9628\n",
      "Epoch [18][20]\t Batch [150][550]\t Training Loss 0.0559\t Accuracy 0.9624\n",
      "Epoch [18][20]\t Batch [200][550]\t Training Loss 0.0554\t Accuracy 0.9631\n",
      "Epoch [18][20]\t Batch [250][550]\t Training Loss 0.0551\t Accuracy 0.9633\n",
      "Epoch [18][20]\t Batch [300][550]\t Training Loss 0.0557\t Accuracy 0.9628\n",
      "Epoch [18][20]\t Batch [350][550]\t Training Loss 0.0557\t Accuracy 0.9630\n",
      "Epoch [18][20]\t Batch [400][550]\t Training Loss 0.0558\t Accuracy 0.9629\n",
      "Epoch [18][20]\t Batch [450][550]\t Training Loss 0.0558\t Accuracy 0.9630\n",
      "Epoch [18][20]\t Batch [500][550]\t Training Loss 0.0562\t Accuracy 0.9624\n",
      "\n",
      "Epoch [18]\t Average training loss 0.0561\t Average training accuracy 0.9624\n",
      "Epoch [18]\t Average validation loss 0.0520\t Average validation accuracy 0.9688\n",
      "\n",
      "Epoch [19][20]\t Batch [0][550]\t Training Loss 0.0465\t Accuracy 0.9700\n",
      "Epoch [19][20]\t Batch [50][550]\t Training Loss 0.0526\t Accuracy 0.9665\n",
      "Epoch [19][20]\t Batch [100][550]\t Training Loss 0.0542\t Accuracy 0.9635\n",
      "Epoch [19][20]\t Batch [150][550]\t Training Loss 0.0552\t Accuracy 0.9634\n",
      "Epoch [19][20]\t Batch [200][550]\t Training Loss 0.0547\t Accuracy 0.9640\n",
      "Epoch [19][20]\t Batch [250][550]\t Training Loss 0.0544\t Accuracy 0.9641\n",
      "Epoch [19][20]\t Batch [300][550]\t Training Loss 0.0550\t Accuracy 0.9635\n",
      "Epoch [19][20]\t Batch [350][550]\t Training Loss 0.0550\t Accuracy 0.9636\n",
      "Epoch [19][20]\t Batch [400][550]\t Training Loss 0.0551\t Accuracy 0.9636\n",
      "Epoch [19][20]\t Batch [450][550]\t Training Loss 0.0551\t Accuracy 0.9635\n",
      "Epoch [19][20]\t Batch [500][550]\t Training Loss 0.0555\t Accuracy 0.9629\n",
      "\n",
      "Epoch [19]\t Average training loss 0.0554\t Average training accuracy 0.9630\n",
      "Epoch [19]\t Average validation loss 0.0515\t Average validation accuracy 0.9702\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reluMLP, relu_loss, relu_acc = train(reluMLP, criterion, sgd, data_train, max_epoch, batch_size, disp_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing...\n",
      "The test accuracy is 0.9622.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test(reluMLP, criterion, data_test, batch_size, disp_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_loss_and_acc({'Sigmoid': [sigmoid_loss, sigmoid_acc],\n",
    "                   'relu': [relu_loss, relu_acc]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. MLP with Softmax Cross-Entropy Loss\n",
    "In part-2, you need to train a MLP with **Softmax Cross-Entropy Loss**.  \n",
    "**Sigmoid Activation Function** and **ReLU Activation Function** will be used respectively again.\n",
    "### TODO\n",
    "Before executing the following code, you should complete **criterion/softmax_cross_entropy_loss.py**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from criterion import SoftmaxCrossEntropyLossLayer\n",
    "\n",
    "\n",
    "criterion = SoftmaxCrossEntropyLossLayer()\n",
    "\n",
    "sgd = SGD(learning_rate_SGD, weight_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 MLP with Softmax Cross-Entropy Loss and Sigmoid Activation Function\n",
    "Build and train a MLP contraining one hidden layer with 128 units using Sigmoid activation function and Softmax cross-entropy loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoidMLP = Network()\n",
    "# Build MLP with FCLayer and SigmoidLayer\n",
    "# 128 is the number of hidden units, you can change by your own\n",
    "sigmoidMLP.add(FCLayer(784, 128))\n",
    "sigmoidMLP.add(SigmoidLayer())\n",
    "sigmoidMLP.add(FCLayer(128, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Research\\Yu Lab\\NLP学习班\\作业\\第一次作业\\附加题\\homework-1\\homework1-mlp\\solver.py:14: DatasetV1.make_one_shot_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_one_shot_iterator(dataset)`.\n",
      "WARNING:tensorflow:From D:\\Research\\Yu Lab\\NLP学习班\\作业\\第一次作业\\附加题\\homework-1\\homework1-mlp\\solver.py:15: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\Research\\Yu Lab\\NLP学习班\\作业\\第一次作业\\附加题\\homework-1\\homework1-mlp\\solver.py:16: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "Epoch [0][20]\t Batch [0][550]\t Training Loss 2.5050\t Accuracy 0.1000\n",
      "Epoch [0][20]\t Batch [50][550]\t Training Loss 1.8384\t Accuracy 0.5282\n",
      "Epoch [0][20]\t Batch [100][550]\t Training Loss 1.5177\t Accuracy 0.6511\n",
      "Epoch [0][20]\t Batch [150][550]\t Training Loss 1.3273\t Accuracy 0.7000\n",
      "Epoch [0][20]\t Batch [200][550]\t Training Loss 1.1946\t Accuracy 0.7322\n",
      "Epoch [0][20]\t Batch [250][550]\t Training Loss 1.0925\t Accuracy 0.7551\n",
      "Epoch [0][20]\t Batch [300][550]\t Training Loss 1.0147\t Accuracy 0.7722\n",
      "Epoch [0][20]\t Batch [350][550]\t Training Loss 0.9580\t Accuracy 0.7835\n",
      "Epoch [0][20]\t Batch [400][550]\t Training Loss 0.9068\t Accuracy 0.7943\n",
      "Epoch [0][20]\t Batch [450][550]\t Training Loss 0.8655\t Accuracy 0.8021\n",
      "Epoch [0][20]\t Batch [500][550]\t Training Loss 0.8305\t Accuracy 0.8086\n",
      "\n",
      "Epoch [0]\t Average training loss 0.7998\t Average training accuracy 0.8147\n",
      "Epoch [0]\t Average validation loss 0.3913\t Average validation accuracy 0.9104\n",
      "\n",
      "Epoch [1][20]\t Batch [0][550]\t Training Loss 0.4546\t Accuracy 0.9100\n",
      "Epoch [1][20]\t Batch [50][550]\t Training Loss 0.4417\t Accuracy 0.8910\n",
      "Epoch [1][20]\t Batch [100][550]\t Training Loss 0.4437\t Accuracy 0.8891\n",
      "Epoch [1][20]\t Batch [150][550]\t Training Loss 0.4505\t Accuracy 0.8848\n",
      "Epoch [1][20]\t Batch [200][550]\t Training Loss 0.4452\t Accuracy 0.8860\n",
      "Epoch [1][20]\t Batch [250][550]\t Training Loss 0.4392\t Accuracy 0.8869\n",
      "Epoch [1][20]\t Batch [300][550]\t Training Loss 0.4351\t Accuracy 0.8873\n",
      "Epoch [1][20]\t Batch [350][550]\t Training Loss 0.4353\t Accuracy 0.8865\n",
      "Epoch [1][20]\t Batch [400][550]\t Training Loss 0.4313\t Accuracy 0.8870\n",
      "Epoch [1][20]\t Batch [450][550]\t Training Loss 0.4287\t Accuracy 0.8872\n",
      "Epoch [1][20]\t Batch [500][550]\t Training Loss 0.4267\t Accuracy 0.8872\n",
      "\n",
      "Epoch [1]\t Average training loss 0.4239\t Average training accuracy 0.8875\n",
      "Epoch [1]\t Average validation loss 0.3104\t Average validation accuracy 0.9232\n",
      "\n",
      "Epoch [2][20]\t Batch [0][550]\t Training Loss 0.3558\t Accuracy 0.9500\n",
      "Epoch [2][20]\t Batch [50][550]\t Training Loss 0.3562\t Accuracy 0.9049\n",
      "Epoch [2][20]\t Batch [100][550]\t Training Loss 0.3650\t Accuracy 0.9013\n",
      "Epoch [2][20]\t Batch [150][550]\t Training Loss 0.3761\t Accuracy 0.8981\n",
      "Epoch [2][20]\t Batch [200][550]\t Training Loss 0.3730\t Accuracy 0.8990\n",
      "Epoch [2][20]\t Batch [250][550]\t Training Loss 0.3704\t Accuracy 0.9002\n",
      "Epoch [2][20]\t Batch [300][550]\t Training Loss 0.3693\t Accuracy 0.9001\n",
      "Epoch [2][20]\t Batch [350][550]\t Training Loss 0.3716\t Accuracy 0.8988\n",
      "Epoch [2][20]\t Batch [400][550]\t Training Loss 0.3701\t Accuracy 0.8990\n",
      "Epoch [2][20]\t Batch [450][550]\t Training Loss 0.3697\t Accuracy 0.8992\n",
      "Epoch [2][20]\t Batch [500][550]\t Training Loss 0.3698\t Accuracy 0.8988\n",
      "\n",
      "Epoch [2]\t Average training loss 0.3691\t Average training accuracy 0.8989\n",
      "Epoch [2]\t Average validation loss 0.2817\t Average validation accuracy 0.9286\n",
      "\n",
      "Epoch [3][20]\t Batch [0][550]\t Training Loss 0.3149\t Accuracy 0.9500\n",
      "Epoch [3][20]\t Batch [50][550]\t Training Loss 0.3230\t Accuracy 0.9129\n",
      "Epoch [3][20]\t Batch [100][550]\t Training Loss 0.3334\t Accuracy 0.9095\n",
      "Epoch [3][20]\t Batch [150][550]\t Training Loss 0.3455\t Accuracy 0.9058\n",
      "Epoch [3][20]\t Batch [200][550]\t Training Loss 0.3425\t Accuracy 0.9066\n",
      "Epoch [3][20]\t Batch [250][550]\t Training Loss 0.3408\t Accuracy 0.9074\n",
      "Epoch [3][20]\t Batch [300][550]\t Training Loss 0.3405\t Accuracy 0.9077\n",
      "Epoch [3][20]\t Batch [350][550]\t Training Loss 0.3432\t Accuracy 0.9061\n",
      "Epoch [3][20]\t Batch [400][550]\t Training Loss 0.3425\t Accuracy 0.9059\n",
      "Epoch [3][20]\t Batch [450][550]\t Training Loss 0.3427\t Accuracy 0.9059\n",
      "Epoch [3][20]\t Batch [500][550]\t Training Loss 0.3434\t Accuracy 0.9055\n",
      "\n",
      "Epoch [3]\t Average training loss 0.3433\t Average training accuracy 0.9053\n",
      "Epoch [3]\t Average validation loss 0.2663\t Average validation accuracy 0.9322\n",
      "\n",
      "Epoch [4][20]\t Batch [0][550]\t Training Loss 0.2915\t Accuracy 0.9500\n",
      "Epoch [4][20]\t Batch [50][550]\t Training Loss 0.3043\t Accuracy 0.9194\n",
      "Epoch [4][20]\t Batch [100][550]\t Training Loss 0.3152\t Accuracy 0.9153\n",
      "Epoch [4][20]\t Batch [150][550]\t Training Loss 0.3276\t Accuracy 0.9107\n",
      "Epoch [4][20]\t Batch [200][550]\t Training Loss 0.3246\t Accuracy 0.9112\n",
      "Epoch [4][20]\t Batch [250][550]\t Training Loss 0.3234\t Accuracy 0.9121\n",
      "Epoch [4][20]\t Batch [300][550]\t Training Loss 0.3233\t Accuracy 0.9124\n",
      "Epoch [4][20]\t Batch [350][550]\t Training Loss 0.3260\t Accuracy 0.9108\n",
      "Epoch [4][20]\t Batch [400][550]\t Training Loss 0.3257\t Accuracy 0.9106\n",
      "Epoch [4][20]\t Batch [450][550]\t Training Loss 0.3262\t Accuracy 0.9103\n",
      "Epoch [4][20]\t Batch [500][550]\t Training Loss 0.3271\t Accuracy 0.9098\n",
      "\n",
      "Epoch [4]\t Average training loss 0.3273\t Average training accuracy 0.9095\n",
      "Epoch [4]\t Average validation loss 0.2561\t Average validation accuracy 0.9350\n",
      "\n",
      "Epoch [5][20]\t Batch [0][550]\t Training Loss 0.2760\t Accuracy 0.9500\n",
      "Epoch [5][20]\t Batch [50][550]\t Training Loss 0.2919\t Accuracy 0.9216\n",
      "Epoch [5][20]\t Batch [100][550]\t Training Loss 0.3028\t Accuracy 0.9184\n",
      "Epoch [5][20]\t Batch [150][550]\t Training Loss 0.3152\t Accuracy 0.9136\n",
      "Epoch [5][20]\t Batch [200][550]\t Training Loss 0.3122\t Accuracy 0.9141\n",
      "Epoch [5][20]\t Batch [250][550]\t Training Loss 0.3113\t Accuracy 0.9149\n",
      "Epoch [5][20]\t Batch [300][550]\t Training Loss 0.3113\t Accuracy 0.9156\n",
      "Epoch [5][20]\t Batch [350][550]\t Training Loss 0.3139\t Accuracy 0.9140\n",
      "Epoch [5][20]\t Batch [400][550]\t Training Loss 0.3138\t Accuracy 0.9138\n",
      "Epoch [5][20]\t Batch [450][550]\t Training Loss 0.3145\t Accuracy 0.9135\n",
      "Epoch [5][20]\t Batch [500][550]\t Training Loss 0.3155\t Accuracy 0.9132\n",
      "\n",
      "Epoch [5]\t Average training loss 0.3159\t Average training accuracy 0.9129\n",
      "Epoch [5]\t Average validation loss 0.2486\t Average validation accuracy 0.9368\n",
      "\n",
      "Epoch [6][20]\t Batch [0][550]\t Training Loss 0.2649\t Accuracy 0.9500\n",
      "Epoch [6][20]\t Batch [50][550]\t Training Loss 0.2826\t Accuracy 0.9245\n",
      "Epoch [6][20]\t Batch [100][550]\t Training Loss 0.2935\t Accuracy 0.9215\n",
      "Epoch [6][20]\t Batch [150][550]\t Training Loss 0.3058\t Accuracy 0.9168\n",
      "Epoch [6][20]\t Batch [200][550]\t Training Loss 0.3028\t Accuracy 0.9172\n",
      "Epoch [6][20]\t Batch [250][550]\t Training Loss 0.3020\t Accuracy 0.9178\n",
      "Epoch [6][20]\t Batch [300][550]\t Training Loss 0.3020\t Accuracy 0.9184\n",
      "Epoch [6][20]\t Batch [350][550]\t Training Loss 0.3046\t Accuracy 0.9168\n",
      "Epoch [6][20]\t Batch [400][550]\t Training Loss 0.3046\t Accuracy 0.9164\n",
      "Epoch [6][20]\t Batch [450][550]\t Training Loss 0.3054\t Accuracy 0.9161\n",
      "Epoch [6][20]\t Batch [500][550]\t Training Loss 0.3065\t Accuracy 0.9159\n",
      "\n",
      "Epoch [6]\t Average training loss 0.3069\t Average training accuracy 0.9157\n",
      "Epoch [6]\t Average validation loss 0.2426\t Average validation accuracy 0.9382\n",
      "\n",
      "Epoch [7][20]\t Batch [0][550]\t Training Loss 0.2564\t Accuracy 0.9500\n",
      "Epoch [7][20]\t Batch [50][550]\t Training Loss 0.2751\t Accuracy 0.9273\n",
      "Epoch [7][20]\t Batch [100][550]\t Training Loss 0.2859\t Accuracy 0.9239\n",
      "Epoch [7][20]\t Batch [150][550]\t Training Loss 0.2980\t Accuracy 0.9196\n",
      "Epoch [7][20]\t Batch [200][550]\t Training Loss 0.2950\t Accuracy 0.9200\n",
      "Epoch [7][20]\t Batch [250][550]\t Training Loss 0.2944\t Accuracy 0.9205\n",
      "Epoch [7][20]\t Batch [300][550]\t Training Loss 0.2945\t Accuracy 0.9210\n",
      "Epoch [7][20]\t Batch [350][550]\t Training Loss 0.2970\t Accuracy 0.9193\n",
      "Epoch [7][20]\t Batch [400][550]\t Training Loss 0.2971\t Accuracy 0.9191\n",
      "Epoch [7][20]\t Batch [450][550]\t Training Loss 0.2979\t Accuracy 0.9187\n",
      "Epoch [7][20]\t Batch [500][550]\t Training Loss 0.2990\t Accuracy 0.9185\n",
      "\n",
      "Epoch [7]\t Average training loss 0.2995\t Average training accuracy 0.9183\n",
      "Epoch [7]\t Average validation loss 0.2375\t Average validation accuracy 0.9390\n",
      "\n",
      "Epoch [8][20]\t Batch [0][550]\t Training Loss 0.2497\t Accuracy 0.9500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8][20]\t Batch [50][550]\t Training Loss 0.2688\t Accuracy 0.9290\n",
      "Epoch [8][20]\t Batch [100][550]\t Training Loss 0.2794\t Accuracy 0.9256\n",
      "Epoch [8][20]\t Batch [150][550]\t Training Loss 0.2913\t Accuracy 0.9214\n",
      "Epoch [8][20]\t Batch [200][550]\t Training Loss 0.2884\t Accuracy 0.9217\n",
      "Epoch [8][20]\t Batch [250][550]\t Training Loss 0.2878\t Accuracy 0.9221\n",
      "Epoch [8][20]\t Batch [300][550]\t Training Loss 0.2880\t Accuracy 0.9227\n",
      "Epoch [8][20]\t Batch [350][550]\t Training Loss 0.2904\t Accuracy 0.9211\n",
      "Epoch [8][20]\t Batch [400][550]\t Training Loss 0.2906\t Accuracy 0.9208\n",
      "Epoch [8][20]\t Batch [450][550]\t Training Loss 0.2914\t Accuracy 0.9204\n",
      "Epoch [8][20]\t Batch [500][550]\t Training Loss 0.2926\t Accuracy 0.9203\n",
      "\n",
      "Epoch [8]\t Average training loss 0.2931\t Average training accuracy 0.9201\n",
      "Epoch [8]\t Average validation loss 0.2331\t Average validation accuracy 0.9404\n",
      "\n",
      "Epoch [9][20]\t Batch [0][550]\t Training Loss 0.2442\t Accuracy 0.9500\n",
      "Epoch [9][20]\t Batch [50][550]\t Training Loss 0.2633\t Accuracy 0.9310\n",
      "Epoch [9][20]\t Batch [100][550]\t Training Loss 0.2737\t Accuracy 0.9267\n",
      "Epoch [9][20]\t Batch [150][550]\t Training Loss 0.2855\t Accuracy 0.9224\n",
      "Epoch [9][20]\t Batch [200][550]\t Training Loss 0.2826\t Accuracy 0.9229\n",
      "Epoch [9][20]\t Batch [250][550]\t Training Loss 0.2821\t Accuracy 0.9233\n",
      "Epoch [9][20]\t Batch [300][550]\t Training Loss 0.2823\t Accuracy 0.9239\n",
      "Epoch [9][20]\t Batch [350][550]\t Training Loss 0.2846\t Accuracy 0.9224\n",
      "Epoch [9][20]\t Batch [400][550]\t Training Loss 0.2848\t Accuracy 0.9221\n",
      "Epoch [9][20]\t Batch [450][550]\t Training Loss 0.2857\t Accuracy 0.9218\n",
      "Epoch [9][20]\t Batch [500][550]\t Training Loss 0.2869\t Accuracy 0.9216\n",
      "\n",
      "Epoch [9]\t Average training loss 0.2874\t Average training accuracy 0.9215\n",
      "Epoch [9]\t Average validation loss 0.2291\t Average validation accuracy 0.9420\n",
      "\n",
      "Epoch [10][20]\t Batch [0][550]\t Training Loss 0.2396\t Accuracy 0.9500\n",
      "Epoch [10][20]\t Batch [50][550]\t Training Loss 0.2583\t Accuracy 0.9329\n",
      "Epoch [10][20]\t Batch [100][550]\t Training Loss 0.2686\t Accuracy 0.9282\n",
      "Epoch [10][20]\t Batch [150][550]\t Training Loss 0.2802\t Accuracy 0.9241\n",
      "Epoch [10][20]\t Batch [200][550]\t Training Loss 0.2773\t Accuracy 0.9246\n",
      "Epoch [10][20]\t Batch [250][550]\t Training Loss 0.2769\t Accuracy 0.9250\n",
      "Epoch [10][20]\t Batch [300][550]\t Training Loss 0.2772\t Accuracy 0.9255\n",
      "Epoch [10][20]\t Batch [350][550]\t Training Loss 0.2794\t Accuracy 0.9240\n",
      "Epoch [10][20]\t Batch [400][550]\t Training Loss 0.2796\t Accuracy 0.9238\n",
      "Epoch [10][20]\t Batch [450][550]\t Training Loss 0.2805\t Accuracy 0.9235\n",
      "Epoch [10][20]\t Batch [500][550]\t Training Loss 0.2817\t Accuracy 0.9233\n",
      "\n",
      "Epoch [10]\t Average training loss 0.2823\t Average training accuracy 0.9231\n",
      "Epoch [10]\t Average validation loss 0.2255\t Average validation accuracy 0.9430\n",
      "\n",
      "Epoch [11][20]\t Batch [0][550]\t Training Loss 0.2355\t Accuracy 0.9500\n",
      "Epoch [11][20]\t Batch [50][550]\t Training Loss 0.2539\t Accuracy 0.9341\n",
      "Epoch [11][20]\t Batch [100][550]\t Training Loss 0.2640\t Accuracy 0.9296\n",
      "Epoch [11][20]\t Batch [150][550]\t Training Loss 0.2753\t Accuracy 0.9253\n",
      "Epoch [11][20]\t Batch [200][550]\t Training Loss 0.2725\t Accuracy 0.9258\n",
      "Epoch [11][20]\t Batch [250][550]\t Training Loss 0.2722\t Accuracy 0.9263\n",
      "Epoch [11][20]\t Batch [300][550]\t Training Loss 0.2725\t Accuracy 0.9268\n",
      "Epoch [11][20]\t Batch [350][550]\t Training Loss 0.2746\t Accuracy 0.9254\n",
      "Epoch [11][20]\t Batch [400][550]\t Training Loss 0.2749\t Accuracy 0.9252\n",
      "Epoch [11][20]\t Batch [450][550]\t Training Loss 0.2758\t Accuracy 0.9251\n",
      "Epoch [11][20]\t Batch [500][550]\t Training Loss 0.2770\t Accuracy 0.9248\n",
      "\n",
      "Epoch [11]\t Average training loss 0.2776\t Average training accuracy 0.9245\n",
      "Epoch [11]\t Average validation loss 0.2221\t Average validation accuracy 0.9436\n",
      "\n",
      "Epoch [12][20]\t Batch [0][550]\t Training Loss 0.2319\t Accuracy 0.9500\n",
      "Epoch [12][20]\t Batch [50][550]\t Training Loss 0.2497\t Accuracy 0.9357\n",
      "Epoch [12][20]\t Batch [100][550]\t Training Loss 0.2597\t Accuracy 0.9310\n",
      "Epoch [12][20]\t Batch [150][550]\t Training Loss 0.2708\t Accuracy 0.9268\n",
      "Epoch [12][20]\t Batch [200][550]\t Training Loss 0.2681\t Accuracy 0.9273\n",
      "Epoch [12][20]\t Batch [250][550]\t Training Loss 0.2679\t Accuracy 0.9278\n",
      "Epoch [12][20]\t Batch [300][550]\t Training Loss 0.2682\t Accuracy 0.9283\n",
      "Epoch [12][20]\t Batch [350][550]\t Training Loss 0.2703\t Accuracy 0.9270\n",
      "Epoch [12][20]\t Batch [400][550]\t Training Loss 0.2705\t Accuracy 0.9268\n",
      "Epoch [12][20]\t Batch [450][550]\t Training Loss 0.2714\t Accuracy 0.9267\n",
      "Epoch [12][20]\t Batch [500][550]\t Training Loss 0.2727\t Accuracy 0.9263\n",
      "\n",
      "Epoch [12]\t Average training loss 0.2733\t Average training accuracy 0.9262\n",
      "Epoch [12]\t Average validation loss 0.2190\t Average validation accuracy 0.9450\n",
      "\n",
      "Epoch [13][20]\t Batch [0][550]\t Training Loss 0.2287\t Accuracy 0.9500\n",
      "Epoch [13][20]\t Batch [50][550]\t Training Loss 0.2459\t Accuracy 0.9371\n",
      "Epoch [13][20]\t Batch [100][550]\t Training Loss 0.2558\t Accuracy 0.9330\n",
      "Epoch [13][20]\t Batch [150][550]\t Training Loss 0.2667\t Accuracy 0.9283\n",
      "Epoch [13][20]\t Batch [200][550]\t Training Loss 0.2641\t Accuracy 0.9289\n",
      "Epoch [13][20]\t Batch [250][550]\t Training Loss 0.2638\t Accuracy 0.9291\n",
      "Epoch [13][20]\t Batch [300][550]\t Training Loss 0.2642\t Accuracy 0.9295\n",
      "Epoch [13][20]\t Batch [350][550]\t Training Loss 0.2662\t Accuracy 0.9284\n",
      "Epoch [13][20]\t Batch [400][550]\t Training Loss 0.2665\t Accuracy 0.9282\n",
      "Epoch [13][20]\t Batch [450][550]\t Training Loss 0.2674\t Accuracy 0.9279\n",
      "Epoch [13][20]\t Batch [500][550]\t Training Loss 0.2686\t Accuracy 0.9275\n",
      "\n",
      "Epoch [13]\t Average training loss 0.2692\t Average training accuracy 0.9273\n",
      "Epoch [13]\t Average validation loss 0.2162\t Average validation accuracy 0.9454\n",
      "\n",
      "Epoch [14][20]\t Batch [0][550]\t Training Loss 0.2258\t Accuracy 0.9500\n",
      "Epoch [14][20]\t Batch [50][550]\t Training Loss 0.2424\t Accuracy 0.9382\n",
      "Epoch [14][20]\t Batch [100][550]\t Training Loss 0.2521\t Accuracy 0.9344\n",
      "Epoch [14][20]\t Batch [150][550]\t Training Loss 0.2628\t Accuracy 0.9297\n",
      "Epoch [14][20]\t Batch [200][550]\t Training Loss 0.2603\t Accuracy 0.9302\n",
      "Epoch [14][20]\t Batch [250][550]\t Training Loss 0.2601\t Accuracy 0.9304\n",
      "Epoch [14][20]\t Batch [300][550]\t Training Loss 0.2604\t Accuracy 0.9307\n",
      "Epoch [14][20]\t Batch [350][550]\t Training Loss 0.2624\t Accuracy 0.9298\n",
      "Epoch [14][20]\t Batch [400][550]\t Training Loss 0.2627\t Accuracy 0.9296\n",
      "Epoch [14][20]\t Batch [450][550]\t Training Loss 0.2636\t Accuracy 0.9292\n",
      "Epoch [14][20]\t Batch [500][550]\t Training Loss 0.2649\t Accuracy 0.9288\n",
      "\n",
      "Epoch [14]\t Average training loss 0.2655\t Average training accuracy 0.9286\n",
      "Epoch [14]\t Average validation loss 0.2135\t Average validation accuracy 0.9464\n",
      "\n",
      "Epoch [15][20]\t Batch [0][550]\t Training Loss 0.2231\t Accuracy 0.9500\n",
      "Epoch [15][20]\t Batch [50][550]\t Training Loss 0.2391\t Accuracy 0.9396\n",
      "Epoch [15][20]\t Batch [100][550]\t Training Loss 0.2486\t Accuracy 0.9356\n",
      "Epoch [15][20]\t Batch [150][550]\t Training Loss 0.2592\t Accuracy 0.9312\n",
      "Epoch [15][20]\t Batch [200][550]\t Training Loss 0.2567\t Accuracy 0.9316\n",
      "Epoch [15][20]\t Batch [250][550]\t Training Loss 0.2566\t Accuracy 0.9316\n",
      "Epoch [15][20]\t Batch [300][550]\t Training Loss 0.2570\t Accuracy 0.9319\n",
      "Epoch [15][20]\t Batch [350][550]\t Training Loss 0.2589\t Accuracy 0.9309\n",
      "Epoch [15][20]\t Batch [400][550]\t Training Loss 0.2592\t Accuracy 0.9308\n",
      "Epoch [15][20]\t Batch [450][550]\t Training Loss 0.2601\t Accuracy 0.9304\n",
      "Epoch [15][20]\t Batch [500][550]\t Training Loss 0.2614\t Accuracy 0.9300\n",
      "\n",
      "Epoch [15]\t Average training loss 0.2620\t Average training accuracy 0.9297\n",
      "Epoch [15]\t Average validation loss 0.2110\t Average validation accuracy 0.9470\n",
      "\n",
      "Epoch [16][20]\t Batch [0][550]\t Training Loss 0.2206\t Accuracy 0.9600\n",
      "Epoch [16][20]\t Batch [50][550]\t Training Loss 0.2359\t Accuracy 0.9406\n",
      "Epoch [16][20]\t Batch [100][550]\t Training Loss 0.2454\t Accuracy 0.9362\n",
      "Epoch [16][20]\t Batch [150][550]\t Training Loss 0.2558\t Accuracy 0.9318\n",
      "Epoch [16][20]\t Batch [200][550]\t Training Loss 0.2534\t Accuracy 0.9323\n",
      "Epoch [16][20]\t Batch [250][550]\t Training Loss 0.2533\t Accuracy 0.9324\n",
      "Epoch [16][20]\t Batch [300][550]\t Training Loss 0.2537\t Accuracy 0.9328\n",
      "Epoch [16][20]\t Batch [350][550]\t Training Loss 0.2556\t Accuracy 0.9317\n",
      "Epoch [16][20]\t Batch [400][550]\t Training Loss 0.2559\t Accuracy 0.9317\n",
      "Epoch [16][20]\t Batch [450][550]\t Training Loss 0.2568\t Accuracy 0.9313\n",
      "Epoch [16][20]\t Batch [500][550]\t Training Loss 0.2581\t Accuracy 0.9308\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [16]\t Average training loss 0.2587\t Average training accuracy 0.9307\n",
      "Epoch [16]\t Average validation loss 0.2087\t Average validation accuracy 0.9474\n",
      "\n",
      "Epoch [17][20]\t Batch [0][550]\t Training Loss 0.2183\t Accuracy 0.9600\n",
      "Epoch [17][20]\t Batch [50][550]\t Training Loss 0.2330\t Accuracy 0.9408\n",
      "Epoch [17][20]\t Batch [100][550]\t Training Loss 0.2424\t Accuracy 0.9369\n",
      "Epoch [17][20]\t Batch [150][550]\t Training Loss 0.2526\t Accuracy 0.9328\n",
      "Epoch [17][20]\t Batch [200][550]\t Training Loss 0.2503\t Accuracy 0.9332\n",
      "Epoch [17][20]\t Batch [250][550]\t Training Loss 0.2502\t Accuracy 0.9334\n",
      "Epoch [17][20]\t Batch [300][550]\t Training Loss 0.2507\t Accuracy 0.9337\n",
      "Epoch [17][20]\t Batch [350][550]\t Training Loss 0.2525\t Accuracy 0.9326\n",
      "Epoch [17][20]\t Batch [400][550]\t Training Loss 0.2528\t Accuracy 0.9326\n",
      "Epoch [17][20]\t Batch [450][550]\t Training Loss 0.2537\t Accuracy 0.9322\n",
      "Epoch [17][20]\t Batch [500][550]\t Training Loss 0.2550\t Accuracy 0.9318\n",
      "\n",
      "Epoch [17]\t Average training loss 0.2556\t Average training accuracy 0.9316\n",
      "Epoch [17]\t Average validation loss 0.2065\t Average validation accuracy 0.9488\n",
      "\n",
      "Epoch [18][20]\t Batch [0][550]\t Training Loss 0.2162\t Accuracy 0.9600\n",
      "Epoch [18][20]\t Batch [50][550]\t Training Loss 0.2303\t Accuracy 0.9416\n",
      "Epoch [18][20]\t Batch [100][550]\t Training Loss 0.2396\t Accuracy 0.9380\n",
      "Epoch [18][20]\t Batch [150][550]\t Training Loss 0.2496\t Accuracy 0.9340\n",
      "Epoch [18][20]\t Batch [200][550]\t Training Loss 0.2474\t Accuracy 0.9343\n",
      "Epoch [18][20]\t Batch [250][550]\t Training Loss 0.2473\t Accuracy 0.9345\n",
      "Epoch [18][20]\t Batch [300][550]\t Training Loss 0.2478\t Accuracy 0.9347\n",
      "Epoch [18][20]\t Batch [350][550]\t Training Loss 0.2496\t Accuracy 0.9337\n",
      "Epoch [18][20]\t Batch [400][550]\t Training Loss 0.2499\t Accuracy 0.9336\n",
      "Epoch [18][20]\t Batch [450][550]\t Training Loss 0.2508\t Accuracy 0.9332\n",
      "Epoch [18][20]\t Batch [500][550]\t Training Loss 0.2521\t Accuracy 0.9328\n",
      "\n",
      "Epoch [18]\t Average training loss 0.2527\t Average training accuracy 0.9326\n",
      "Epoch [18]\t Average validation loss 0.2044\t Average validation accuracy 0.9490\n",
      "\n",
      "Epoch [19][20]\t Batch [0][550]\t Training Loss 0.2141\t Accuracy 0.9600\n",
      "Epoch [19][20]\t Batch [50][550]\t Training Loss 0.2277\t Accuracy 0.9422\n",
      "Epoch [19][20]\t Batch [100][550]\t Training Loss 0.2370\t Accuracy 0.9387\n",
      "Epoch [19][20]\t Batch [150][550]\t Training Loss 0.2468\t Accuracy 0.9346\n",
      "Epoch [19][20]\t Batch [200][550]\t Training Loss 0.2446\t Accuracy 0.9350\n",
      "Epoch [19][20]\t Batch [250][550]\t Training Loss 0.2446\t Accuracy 0.9350\n",
      "Epoch [19][20]\t Batch [300][550]\t Training Loss 0.2451\t Accuracy 0.9353\n",
      "Epoch [19][20]\t Batch [350][550]\t Training Loss 0.2469\t Accuracy 0.9344\n",
      "Epoch [19][20]\t Batch [400][550]\t Training Loss 0.2471\t Accuracy 0.9343\n",
      "Epoch [19][20]\t Batch [450][550]\t Training Loss 0.2481\t Accuracy 0.9339\n",
      "Epoch [19][20]\t Batch [500][550]\t Training Loss 0.2494\t Accuracy 0.9336\n",
      "\n",
      "Epoch [19]\t Average training loss 0.2500\t Average training accuracy 0.9335\n",
      "Epoch [19]\t Average validation loss 0.2025\t Average validation accuracy 0.9502\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sigmoidMLP, sigmoid_loss, sigmoid_acc = train(sigmoidMLP, criterion, sgd, data_train, max_epoch, batch_size, disp_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing...\n",
      "The test accuracy is 0.9356.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test(sigmoidMLP, criterion, data_test, batch_size, disp_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 MLP with Softmax Cross-Entropy Loss and ReLU Activation Function\n",
    "Build and train a MLP contraining one hidden layer with 128 units using ReLU activation function and Softmax cross-entropy loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "reluMLP = Network()\n",
    "# Build ReLUMLP with FCLayer and ReLULayer\n",
    "# 128 is the number of hidden units, you can change by your own\n",
    "reluMLP.add(FCLayer(784, 128))\n",
    "reluMLP.add(ReLULayer())\n",
    "reluMLP.add(FCLayer(128, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0][20]\t Batch [0][550]\t Training Loss 2.5321\t Accuracy 0.1000\n",
      "Epoch [0][20]\t Batch [50][550]\t Training Loss 1.0179\t Accuracy 0.7261\n",
      "Epoch [0][20]\t Batch [100][550]\t Training Loss 0.7633\t Accuracy 0.7958\n",
      "Epoch [0][20]\t Batch [150][550]\t Training Loss 0.6569\t Accuracy 0.8219\n",
      "Epoch [0][20]\t Batch [200][550]\t Training Loss 0.5872\t Accuracy 0.8406\n",
      "Epoch [0][20]\t Batch [250][550]\t Training Loss 0.5390\t Accuracy 0.8531\n",
      "Epoch [0][20]\t Batch [300][550]\t Training Loss 0.5044\t Accuracy 0.8618\n",
      "Epoch [0][20]\t Batch [350][550]\t Training Loss 0.4801\t Accuracy 0.8675\n",
      "Epoch [0][20]\t Batch [400][550]\t Training Loss 0.4583\t Accuracy 0.8732\n",
      "Epoch [0][20]\t Batch [450][550]\t Training Loss 0.4410\t Accuracy 0.8778\n",
      "Epoch [0][20]\t Batch [500][550]\t Training Loss 0.4273\t Accuracy 0.8813\n",
      "\n",
      "Epoch [0]\t Average training loss 0.4146\t Average training accuracy 0.8847\n",
      "Epoch [0]\t Average validation loss 0.2117\t Average validation accuracy 0.9456\n",
      "\n",
      "Epoch [1][20]\t Batch [0][550]\t Training Loss 0.2828\t Accuracy 0.9500\n",
      "Epoch [1][20]\t Batch [50][550]\t Training Loss 0.2383\t Accuracy 0.9361\n",
      "Epoch [1][20]\t Batch [100][550]\t Training Loss 0.2471\t Accuracy 0.9328\n",
      "Epoch [1][20]\t Batch [150][550]\t Training Loss 0.2538\t Accuracy 0.9297\n",
      "Epoch [1][20]\t Batch [200][550]\t Training Loss 0.2483\t Accuracy 0.9313\n",
      "Epoch [1][20]\t Batch [250][550]\t Training Loss 0.2454\t Accuracy 0.9320\n",
      "Epoch [1][20]\t Batch [300][550]\t Training Loss 0.2434\t Accuracy 0.9326\n",
      "Epoch [1][20]\t Batch [350][550]\t Training Loss 0.2424\t Accuracy 0.9329\n",
      "Epoch [1][20]\t Batch [400][550]\t Training Loss 0.2400\t Accuracy 0.9337\n",
      "Epoch [1][20]\t Batch [450][550]\t Training Loss 0.2385\t Accuracy 0.9341\n",
      "Epoch [1][20]\t Batch [500][550]\t Training Loss 0.2382\t Accuracy 0.9339\n",
      "\n",
      "Epoch [1]\t Average training loss 0.2364\t Average training accuracy 0.9344\n",
      "Epoch [1]\t Average validation loss 0.1684\t Average validation accuracy 0.9572\n",
      "\n",
      "Epoch [2][20]\t Batch [0][550]\t Training Loss 0.2027\t Accuracy 0.9600\n",
      "Epoch [2][20]\t Batch [50][550]\t Training Loss 0.1848\t Accuracy 0.9512\n",
      "Epoch [2][20]\t Batch [100][550]\t Training Loss 0.1945\t Accuracy 0.9471\n",
      "Epoch [2][20]\t Batch [150][550]\t Training Loss 0.1997\t Accuracy 0.9450\n",
      "Epoch [2][20]\t Batch [200][550]\t Training Loss 0.1963\t Accuracy 0.9457\n",
      "Epoch [2][20]\t Batch [250][550]\t Training Loss 0.1948\t Accuracy 0.9462\n",
      "Epoch [2][20]\t Batch [300][550]\t Training Loss 0.1941\t Accuracy 0.9466\n",
      "Epoch [2][20]\t Batch [350][550]\t Training Loss 0.1940\t Accuracy 0.9466\n",
      "Epoch [2][20]\t Batch [400][550]\t Training Loss 0.1925\t Accuracy 0.9471\n",
      "Epoch [2][20]\t Batch [450][550]\t Training Loss 0.1919\t Accuracy 0.9472\n",
      "Epoch [2][20]\t Batch [500][550]\t Training Loss 0.1925\t Accuracy 0.9468\n",
      "\n",
      "Epoch [2]\t Average training loss 0.1915\t Average training accuracy 0.9472\n",
      "Epoch [2]\t Average validation loss 0.1463\t Average validation accuracy 0.9632\n",
      "\n",
      "Epoch [3][20]\t Batch [0][550]\t Training Loss 0.1661\t Accuracy 0.9600\n",
      "Epoch [3][20]\t Batch [50][550]\t Training Loss 0.1556\t Accuracy 0.9604\n",
      "Epoch [3][20]\t Batch [100][550]\t Training Loss 0.1652\t Accuracy 0.9557\n",
      "Epoch [3][20]\t Batch [150][550]\t Training Loss 0.1689\t Accuracy 0.9536\n",
      "Epoch [3][20]\t Batch [200][550]\t Training Loss 0.1664\t Accuracy 0.9545\n",
      "Epoch [3][20]\t Batch [250][550]\t Training Loss 0.1653\t Accuracy 0.9547\n",
      "Epoch [3][20]\t Batch [300][550]\t Training Loss 0.1653\t Accuracy 0.9547\n",
      "Epoch [3][20]\t Batch [350][550]\t Training Loss 0.1655\t Accuracy 0.9547\n",
      "Epoch [3][20]\t Batch [400][550]\t Training Loss 0.1644\t Accuracy 0.9551\n",
      "Epoch [3][20]\t Batch [450][550]\t Training Loss 0.1642\t Accuracy 0.9552\n",
      "Epoch [3][20]\t Batch [500][550]\t Training Loss 0.1652\t Accuracy 0.9548\n",
      "\n",
      "Epoch [3]\t Average training loss 0.1645\t Average training accuracy 0.9549\n",
      "Epoch [3]\t Average validation loss 0.1324\t Average validation accuracy 0.9672\n",
      "\n",
      "Epoch [4][20]\t Batch [0][550]\t Training Loss 0.1437\t Accuracy 0.9600\n",
      "Epoch [4][20]\t Batch [50][550]\t Training Loss 0.1370\t Accuracy 0.9651\n",
      "Epoch [4][20]\t Batch [100][550]\t Training Loss 0.1461\t Accuracy 0.9610\n",
      "Epoch [4][20]\t Batch [150][550]\t Training Loss 0.1486\t Accuracy 0.9595\n",
      "Epoch [4][20]\t Batch [200][550]\t Training Loss 0.1468\t Accuracy 0.9600\n",
      "Epoch [4][20]\t Batch [250][550]\t Training Loss 0.1458\t Accuracy 0.9600\n",
      "Epoch [4][20]\t Batch [300][550]\t Training Loss 0.1463\t Accuracy 0.9599\n",
      "Epoch [4][20]\t Batch [350][550]\t Training Loss 0.1465\t Accuracy 0.9599\n",
      "Epoch [4][20]\t Batch [400][550]\t Training Loss 0.1458\t Accuracy 0.9604\n",
      "Epoch [4][20]\t Batch [450][550]\t Training Loss 0.1457\t Accuracy 0.9605\n",
      "Epoch [4][20]\t Batch [500][550]\t Training Loss 0.1469\t Accuracy 0.9600\n",
      "\n",
      "Epoch [4]\t Average training loss 0.1464\t Average training accuracy 0.9603\n",
      "Epoch [4]\t Average validation loss 0.1229\t Average validation accuracy 0.9696\n",
      "\n",
      "Epoch [5][20]\t Batch [0][550]\t Training Loss 0.1281\t Accuracy 0.9700\n",
      "Epoch [5][20]\t Batch [50][550]\t Training Loss 0.1238\t Accuracy 0.9704\n",
      "Epoch [5][20]\t Batch [100][550]\t Training Loss 0.1324\t Accuracy 0.9655\n",
      "Epoch [5][20]\t Batch [150][550]\t Training Loss 0.1340\t Accuracy 0.9641\n",
      "Epoch [5][20]\t Batch [200][550]\t Training Loss 0.1327\t Accuracy 0.9645\n",
      "Epoch [5][20]\t Batch [250][550]\t Training Loss 0.1318\t Accuracy 0.9648\n",
      "Epoch [5][20]\t Batch [300][550]\t Training Loss 0.1326\t Accuracy 0.9645\n",
      "Epoch [5][20]\t Batch [350][550]\t Training Loss 0.1329\t Accuracy 0.9643\n",
      "Epoch [5][20]\t Batch [400][550]\t Training Loss 0.1323\t Accuracy 0.9646\n",
      "Epoch [5][20]\t Batch [450][550]\t Training Loss 0.1322\t Accuracy 0.9646\n",
      "Epoch [5][20]\t Batch [500][550]\t Training Loss 0.1335\t Accuracy 0.9640\n",
      "\n",
      "Epoch [5]\t Average training loss 0.1331\t Average training accuracy 0.9644\n",
      "Epoch [5]\t Average validation loss 0.1158\t Average validation accuracy 0.9722\n",
      "\n",
      "Epoch [6][20]\t Batch [0][550]\t Training Loss 0.1163\t Accuracy 0.9700\n",
      "Epoch [6][20]\t Batch [50][550]\t Training Loss 0.1139\t Accuracy 0.9725\n",
      "Epoch [6][20]\t Batch [100][550]\t Training Loss 0.1220\t Accuracy 0.9686\n",
      "Epoch [6][20]\t Batch [150][550]\t Training Loss 0.1229\t Accuracy 0.9681\n",
      "Epoch [6][20]\t Batch [200][550]\t Training Loss 0.1220\t Accuracy 0.9685\n",
      "Epoch [6][20]\t Batch [250][550]\t Training Loss 0.1212\t Accuracy 0.9684\n",
      "Epoch [6][20]\t Batch [300][550]\t Training Loss 0.1220\t Accuracy 0.9678\n",
      "Epoch [6][20]\t Batch [350][550]\t Training Loss 0.1224\t Accuracy 0.9675\n",
      "Epoch [6][20]\t Batch [400][550]\t Training Loss 0.1219\t Accuracy 0.9678\n",
      "Epoch [6][20]\t Batch [450][550]\t Training Loss 0.1219\t Accuracy 0.9678\n",
      "Epoch [6][20]\t Batch [500][550]\t Training Loss 0.1232\t Accuracy 0.9671\n",
      "\n",
      "Epoch [6]\t Average training loss 0.1229\t Average training accuracy 0.9675\n",
      "Epoch [6]\t Average validation loss 0.1107\t Average validation accuracy 0.9734\n",
      "\n",
      "Epoch [7][20]\t Batch [0][550]\t Training Loss 0.1071\t Accuracy 0.9700\n",
      "Epoch [7][20]\t Batch [50][550]\t Training Loss 0.1062\t Accuracy 0.9733\n",
      "Epoch [7][20]\t Batch [100][550]\t Training Loss 0.1138\t Accuracy 0.9703\n",
      "Epoch [7][20]\t Batch [150][550]\t Training Loss 0.1143\t Accuracy 0.9703\n",
      "Epoch [7][20]\t Batch [200][550]\t Training Loss 0.1136\t Accuracy 0.9705\n",
      "Epoch [7][20]\t Batch [250][550]\t Training Loss 0.1128\t Accuracy 0.9705\n",
      "Epoch [7][20]\t Batch [300][550]\t Training Loss 0.1137\t Accuracy 0.9702\n",
      "Epoch [7][20]\t Batch [350][550]\t Training Loss 0.1140\t Accuracy 0.9699\n",
      "Epoch [7][20]\t Batch [400][550]\t Training Loss 0.1137\t Accuracy 0.9701\n",
      "Epoch [7][20]\t Batch [450][550]\t Training Loss 0.1137\t Accuracy 0.9702\n",
      "Epoch [7][20]\t Batch [500][550]\t Training Loss 0.1150\t Accuracy 0.9696\n",
      "\n",
      "Epoch [7]\t Average training loss 0.1147\t Average training accuracy 0.9698\n",
      "Epoch [7]\t Average validation loss 0.1064\t Average validation accuracy 0.9746\n",
      "\n",
      "Epoch [8][20]\t Batch [0][550]\t Training Loss 0.1006\t Accuracy 0.9700\n",
      "Epoch [8][20]\t Batch [50][550]\t Training Loss 0.1002\t Accuracy 0.9751\n",
      "Epoch [8][20]\t Batch [100][550]\t Training Loss 0.1072\t Accuracy 0.9720\n",
      "Epoch [8][20]\t Batch [150][550]\t Training Loss 0.1073\t Accuracy 0.9719\n",
      "Epoch [8][20]\t Batch [200][550]\t Training Loss 0.1068\t Accuracy 0.9722\n",
      "Epoch [8][20]\t Batch [250][550]\t Training Loss 0.1060\t Accuracy 0.9724\n",
      "Epoch [8][20]\t Batch [300][550]\t Training Loss 0.1070\t Accuracy 0.9720\n",
      "Epoch [8][20]\t Batch [350][550]\t Training Loss 0.1073\t Accuracy 0.9718\n",
      "Epoch [8][20]\t Batch [400][550]\t Training Loss 0.1071\t Accuracy 0.9719\n",
      "Epoch [8][20]\t Batch [450][550]\t Training Loss 0.1071\t Accuracy 0.9720\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8][20]\t Batch [500][550]\t Training Loss 0.1083\t Accuracy 0.9714\n",
      "\n",
      "Epoch [8]\t Average training loss 0.1081\t Average training accuracy 0.9715\n",
      "Epoch [8]\t Average validation loss 0.1028\t Average validation accuracy 0.9752\n",
      "\n",
      "Epoch [9][20]\t Batch [0][550]\t Training Loss 0.0952\t Accuracy 0.9800\n",
      "Epoch [9][20]\t Batch [50][550]\t Training Loss 0.0952\t Accuracy 0.9769\n",
      "Epoch [9][20]\t Batch [100][550]\t Training Loss 0.1018\t Accuracy 0.9741\n",
      "Epoch [9][20]\t Batch [150][550]\t Training Loss 0.1015\t Accuracy 0.9739\n",
      "Epoch [9][20]\t Batch [200][550]\t Training Loss 0.1012\t Accuracy 0.9742\n",
      "Epoch [9][20]\t Batch [250][550]\t Training Loss 0.1005\t Accuracy 0.9744\n",
      "Epoch [9][20]\t Batch [300][550]\t Training Loss 0.1015\t Accuracy 0.9741\n",
      "Epoch [9][20]\t Batch [350][550]\t Training Loss 0.1017\t Accuracy 0.9738\n",
      "Epoch [9][20]\t Batch [400][550]\t Training Loss 0.1016\t Accuracy 0.9739\n",
      "Epoch [9][20]\t Batch [450][550]\t Training Loss 0.1016\t Accuracy 0.9740\n",
      "Epoch [9][20]\t Batch [500][550]\t Training Loss 0.1029\t Accuracy 0.9735\n",
      "\n",
      "Epoch [9]\t Average training loss 0.1027\t Average training accuracy 0.9736\n",
      "Epoch [9]\t Average validation loss 0.0997\t Average validation accuracy 0.9766\n",
      "\n",
      "Epoch [10][20]\t Batch [0][550]\t Training Loss 0.0903\t Accuracy 0.9800\n",
      "Epoch [10][20]\t Batch [50][550]\t Training Loss 0.0911\t Accuracy 0.9776\n",
      "Epoch [10][20]\t Batch [100][550]\t Training Loss 0.0972\t Accuracy 0.9757\n",
      "Epoch [10][20]\t Batch [150][550]\t Training Loss 0.0968\t Accuracy 0.9754\n",
      "Epoch [10][20]\t Batch [200][550]\t Training Loss 0.0965\t Accuracy 0.9760\n",
      "Epoch [10][20]\t Batch [250][550]\t Training Loss 0.0959\t Accuracy 0.9760\n",
      "Epoch [10][20]\t Batch [300][550]\t Training Loss 0.0969\t Accuracy 0.9756\n",
      "Epoch [10][20]\t Batch [350][550]\t Training Loss 0.0971\t Accuracy 0.9755\n",
      "Epoch [10][20]\t Batch [400][550]\t Training Loss 0.0970\t Accuracy 0.9756\n",
      "Epoch [10][20]\t Batch [450][550]\t Training Loss 0.0971\t Accuracy 0.9755\n",
      "Epoch [10][20]\t Batch [500][550]\t Training Loss 0.0983\t Accuracy 0.9751\n",
      "\n",
      "Epoch [10]\t Average training loss 0.0981\t Average training accuracy 0.9752\n",
      "Epoch [10]\t Average validation loss 0.0972\t Average validation accuracy 0.9770\n",
      "\n",
      "Epoch [11][20]\t Batch [0][550]\t Training Loss 0.0858\t Accuracy 0.9800\n",
      "Epoch [11][20]\t Batch [50][550]\t Training Loss 0.0876\t Accuracy 0.9788\n",
      "Epoch [11][20]\t Batch [100][550]\t Training Loss 0.0934\t Accuracy 0.9774\n",
      "Epoch [11][20]\t Batch [150][550]\t Training Loss 0.0927\t Accuracy 0.9768\n",
      "Epoch [11][20]\t Batch [200][550]\t Training Loss 0.0926\t Accuracy 0.9770\n",
      "Epoch [11][20]\t Batch [250][550]\t Training Loss 0.0920\t Accuracy 0.9770\n",
      "Epoch [11][20]\t Batch [300][550]\t Training Loss 0.0929\t Accuracy 0.9768\n",
      "Epoch [11][20]\t Batch [350][550]\t Training Loss 0.0932\t Accuracy 0.9765\n",
      "Epoch [11][20]\t Batch [400][550]\t Training Loss 0.0932\t Accuracy 0.9766\n",
      "Epoch [11][20]\t Batch [450][550]\t Training Loss 0.0932\t Accuracy 0.9766\n",
      "Epoch [11][20]\t Batch [500][550]\t Training Loss 0.0944\t Accuracy 0.9762\n",
      "\n",
      "Epoch [11]\t Average training loss 0.0943\t Average training accuracy 0.9763\n",
      "Epoch [11]\t Average validation loss 0.0951\t Average validation accuracy 0.9778\n",
      "\n",
      "Epoch [12][20]\t Batch [0][550]\t Training Loss 0.0827\t Accuracy 0.9800\n",
      "Epoch [12][20]\t Batch [50][550]\t Training Loss 0.0846\t Accuracy 0.9790\n",
      "Epoch [12][20]\t Batch [100][550]\t Training Loss 0.0900\t Accuracy 0.9779\n",
      "Epoch [12][20]\t Batch [150][550]\t Training Loss 0.0893\t Accuracy 0.9772\n",
      "Epoch [12][20]\t Batch [200][550]\t Training Loss 0.0892\t Accuracy 0.9776\n",
      "Epoch [12][20]\t Batch [250][550]\t Training Loss 0.0886\t Accuracy 0.9776\n",
      "Epoch [12][20]\t Batch [300][550]\t Training Loss 0.0896\t Accuracy 0.9776\n",
      "Epoch [12][20]\t Batch [350][550]\t Training Loss 0.0898\t Accuracy 0.9774\n",
      "Epoch [12][20]\t Batch [400][550]\t Training Loss 0.0898\t Accuracy 0.9775\n",
      "Epoch [12][20]\t Batch [450][550]\t Training Loss 0.0899\t Accuracy 0.9775\n",
      "Epoch [12][20]\t Batch [500][550]\t Training Loss 0.0911\t Accuracy 0.9771\n",
      "\n",
      "Epoch [12]\t Average training loss 0.0909\t Average training accuracy 0.9772\n",
      "Epoch [12]\t Average validation loss 0.0933\t Average validation accuracy 0.9790\n",
      "\n",
      "Epoch [13][20]\t Batch [0][550]\t Training Loss 0.0798\t Accuracy 0.9800\n",
      "Epoch [13][20]\t Batch [50][550]\t Training Loss 0.0820\t Accuracy 0.9798\n",
      "Epoch [13][20]\t Batch [100][550]\t Training Loss 0.0872\t Accuracy 0.9787\n",
      "Epoch [13][20]\t Batch [150][550]\t Training Loss 0.0863\t Accuracy 0.9782\n",
      "Epoch [13][20]\t Batch [200][550]\t Training Loss 0.0863\t Accuracy 0.9785\n",
      "Epoch [13][20]\t Batch [250][550]\t Training Loss 0.0857\t Accuracy 0.9786\n",
      "Epoch [13][20]\t Batch [300][550]\t Training Loss 0.0867\t Accuracy 0.9786\n",
      "Epoch [13][20]\t Batch [350][550]\t Training Loss 0.0869\t Accuracy 0.9783\n",
      "Epoch [13][20]\t Batch [400][550]\t Training Loss 0.0870\t Accuracy 0.9783\n",
      "Epoch [13][20]\t Batch [450][550]\t Training Loss 0.0870\t Accuracy 0.9782\n",
      "Epoch [13][20]\t Batch [500][550]\t Training Loss 0.0882\t Accuracy 0.9780\n",
      "\n",
      "Epoch [13]\t Average training loss 0.0881\t Average training accuracy 0.9781\n",
      "Epoch [13]\t Average validation loss 0.0916\t Average validation accuracy 0.9792\n",
      "\n",
      "Epoch [14][20]\t Batch [0][550]\t Training Loss 0.0775\t Accuracy 0.9800\n",
      "Epoch [14][20]\t Batch [50][550]\t Training Loss 0.0797\t Accuracy 0.9802\n",
      "Epoch [14][20]\t Batch [100][550]\t Training Loss 0.0846\t Accuracy 0.9797\n",
      "Epoch [14][20]\t Batch [150][550]\t Training Loss 0.0837\t Accuracy 0.9791\n",
      "Epoch [14][20]\t Batch [200][550]\t Training Loss 0.0837\t Accuracy 0.9795\n",
      "Epoch [14][20]\t Batch [250][550]\t Training Loss 0.0832\t Accuracy 0.9795\n",
      "Epoch [14][20]\t Batch [300][550]\t Training Loss 0.0842\t Accuracy 0.9793\n",
      "Epoch [14][20]\t Batch [350][550]\t Training Loss 0.0844\t Accuracy 0.9791\n",
      "Epoch [14][20]\t Batch [400][550]\t Training Loss 0.0845\t Accuracy 0.9792\n",
      "Epoch [14][20]\t Batch [450][550]\t Training Loss 0.0846\t Accuracy 0.9791\n",
      "Epoch [14][20]\t Batch [500][550]\t Training Loss 0.0857\t Accuracy 0.9788\n",
      "\n",
      "Epoch [14]\t Average training loss 0.0856\t Average training accuracy 0.9788\n",
      "Epoch [14]\t Average validation loss 0.0903\t Average validation accuracy 0.9794\n",
      "\n",
      "Epoch [15][20]\t Batch [0][550]\t Training Loss 0.0752\t Accuracy 0.9900\n",
      "Epoch [15][20]\t Batch [50][550]\t Training Loss 0.0777\t Accuracy 0.9810\n",
      "Epoch [15][20]\t Batch [100][550]\t Training Loss 0.0825\t Accuracy 0.9801\n",
      "Epoch [15][20]\t Batch [150][550]\t Training Loss 0.0815\t Accuracy 0.9797\n",
      "Epoch [15][20]\t Batch [200][550]\t Training Loss 0.0815\t Accuracy 0.9801\n",
      "Epoch [15][20]\t Batch [250][550]\t Training Loss 0.0811\t Accuracy 0.9801\n",
      "Epoch [15][20]\t Batch [300][550]\t Training Loss 0.0820\t Accuracy 0.9800\n",
      "Epoch [15][20]\t Batch [350][550]\t Training Loss 0.0822\t Accuracy 0.9798\n",
      "Epoch [15][20]\t Batch [400][550]\t Training Loss 0.0824\t Accuracy 0.9798\n",
      "Epoch [15][20]\t Batch [450][550]\t Training Loss 0.0824\t Accuracy 0.9797\n",
      "Epoch [15][20]\t Batch [500][550]\t Training Loss 0.0836\t Accuracy 0.9794\n",
      "\n",
      "Epoch [15]\t Average training loss 0.0835\t Average training accuracy 0.9795\n",
      "Epoch [15]\t Average validation loss 0.0889\t Average validation accuracy 0.9794\n",
      "\n",
      "Epoch [16][20]\t Batch [0][550]\t Training Loss 0.0739\t Accuracy 0.9900\n",
      "Epoch [16][20]\t Batch [50][550]\t Training Loss 0.0760\t Accuracy 0.9814\n",
      "Epoch [16][20]\t Batch [100][550]\t Training Loss 0.0805\t Accuracy 0.9807\n",
      "Epoch [16][20]\t Batch [150][550]\t Training Loss 0.0795\t Accuracy 0.9805\n",
      "Epoch [16][20]\t Batch [200][550]\t Training Loss 0.0796\t Accuracy 0.9806\n",
      "Epoch [16][20]\t Batch [250][550]\t Training Loss 0.0791\t Accuracy 0.9807\n",
      "Epoch [16][20]\t Batch [300][550]\t Training Loss 0.0801\t Accuracy 0.9805\n",
      "Epoch [16][20]\t Batch [350][550]\t Training Loss 0.0803\t Accuracy 0.9803\n",
      "Epoch [16][20]\t Batch [400][550]\t Training Loss 0.0805\t Accuracy 0.9803\n",
      "Epoch [16][20]\t Batch [450][550]\t Training Loss 0.0806\t Accuracy 0.9802\n",
      "Epoch [16][20]\t Batch [500][550]\t Training Loss 0.0817\t Accuracy 0.9800\n",
      "\n",
      "Epoch [16]\t Average training loss 0.0816\t Average training accuracy 0.9801\n",
      "Epoch [16]\t Average validation loss 0.0879\t Average validation accuracy 0.9792\n",
      "\n",
      "Epoch [17][20]\t Batch [0][550]\t Training Loss 0.0723\t Accuracy 0.9900\n",
      "Epoch [17][20]\t Batch [50][550]\t Training Loss 0.0745\t Accuracy 0.9825\n",
      "Epoch [17][20]\t Batch [100][550]\t Training Loss 0.0788\t Accuracy 0.9819\n",
      "Epoch [17][20]\t Batch [150][550]\t Training Loss 0.0778\t Accuracy 0.9815\n",
      "Epoch [17][20]\t Batch [200][550]\t Training Loss 0.0779\t Accuracy 0.9816\n",
      "Epoch [17][20]\t Batch [250][550]\t Training Loss 0.0775\t Accuracy 0.9816\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17][20]\t Batch [300][550]\t Training Loss 0.0784\t Accuracy 0.9814\n",
      "Epoch [17][20]\t Batch [350][550]\t Training Loss 0.0786\t Accuracy 0.9812\n",
      "Epoch [17][20]\t Batch [400][550]\t Training Loss 0.0788\t Accuracy 0.9811\n",
      "Epoch [17][20]\t Batch [450][550]\t Training Loss 0.0789\t Accuracy 0.9809\n",
      "Epoch [17][20]\t Batch [500][550]\t Training Loss 0.0800\t Accuracy 0.9806\n",
      "\n",
      "Epoch [17]\t Average training loss 0.0799\t Average training accuracy 0.9806\n",
      "Epoch [17]\t Average validation loss 0.0869\t Average validation accuracy 0.9794\n",
      "\n",
      "Epoch [18][20]\t Batch [0][550]\t Training Loss 0.0705\t Accuracy 0.9900\n",
      "Epoch [18][20]\t Batch [50][550]\t Training Loss 0.0731\t Accuracy 0.9833\n",
      "Epoch [18][20]\t Batch [100][550]\t Training Loss 0.0773\t Accuracy 0.9824\n",
      "Epoch [18][20]\t Batch [150][550]\t Training Loss 0.0763\t Accuracy 0.9820\n",
      "Epoch [18][20]\t Batch [200][550]\t Training Loss 0.0764\t Accuracy 0.9820\n",
      "Epoch [18][20]\t Batch [250][550]\t Training Loss 0.0760\t Accuracy 0.9820\n",
      "Epoch [18][20]\t Batch [300][550]\t Training Loss 0.0770\t Accuracy 0.9819\n",
      "Epoch [18][20]\t Batch [350][550]\t Training Loss 0.0771\t Accuracy 0.9817\n",
      "Epoch [18][20]\t Batch [400][550]\t Training Loss 0.0774\t Accuracy 0.9816\n",
      "Epoch [18][20]\t Batch [450][550]\t Training Loss 0.0774\t Accuracy 0.9814\n",
      "Epoch [18][20]\t Batch [500][550]\t Training Loss 0.0785\t Accuracy 0.9810\n",
      "\n",
      "Epoch [18]\t Average training loss 0.0785\t Average training accuracy 0.9811\n",
      "Epoch [18]\t Average validation loss 0.0861\t Average validation accuracy 0.9794\n",
      "\n",
      "Epoch [19][20]\t Batch [0][550]\t Training Loss 0.0698\t Accuracy 0.9900\n",
      "Epoch [19][20]\t Batch [50][550]\t Training Loss 0.0718\t Accuracy 0.9835\n",
      "Epoch [19][20]\t Batch [100][550]\t Training Loss 0.0760\t Accuracy 0.9825\n",
      "Epoch [19][20]\t Batch [150][550]\t Training Loss 0.0749\t Accuracy 0.9824\n",
      "Epoch [19][20]\t Batch [200][550]\t Training Loss 0.0750\t Accuracy 0.9823\n",
      "Epoch [19][20]\t Batch [250][550]\t Training Loss 0.0746\t Accuracy 0.9823\n",
      "Epoch [19][20]\t Batch [300][550]\t Training Loss 0.0756\t Accuracy 0.9822\n",
      "Epoch [19][20]\t Batch [350][550]\t Training Loss 0.0757\t Accuracy 0.9820\n",
      "Epoch [19][20]\t Batch [400][550]\t Training Loss 0.0760\t Accuracy 0.9819\n",
      "Epoch [19][20]\t Batch [450][550]\t Training Loss 0.0761\t Accuracy 0.9818\n",
      "Epoch [19][20]\t Batch [500][550]\t Training Loss 0.0772\t Accuracy 0.9815\n",
      "\n",
      "Epoch [19]\t Average training loss 0.0771\t Average training accuracy 0.9815\n",
      "Epoch [19]\t Average validation loss 0.0854\t Average validation accuracy 0.9796\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reluMLP, relu_loss, relu_acc = train(reluMLP, criterion, sgd, data_train, max_epoch, batch_size, disp_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing...\n",
      "The test accuracy is 0.9736.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test(reluMLP, criterion, data_test, batch_size, disp_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_loss_and_acc({'Sigmoid': [sigmoid_loss, sigmoid_acc],\n",
    "                   'relu': [relu_loss, relu_acc]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ~~You have finished homework1-mlp, congratulations!~~  \n",
    "\n",
    "**Next, according to the requirements 4) of report:**\n",
    "### **You need to construct a two-hidden-layer MLP, using any activation function and loss function.**\n",
    "\n",
    "**Note: Please insert some new cells blow (using '+' bottom in the toolbar) refer to above codes. Do not modify the former code directly.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
